{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\miniconda3\\envs\\hehe\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from lightning import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from func import FlickrDataset, CLIPDualEncoderModel, Config\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158915, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption_number</th>\n",
       "      <th>caption</th>\n",
       "      <th>id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their ...</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many ...</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Several men in hard hats are operating a giant...</td>\n",
       "      <td>1</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Workers look down from up above on a piece of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image caption_number  \\\n",
       "0  1000092795.jpg              0   \n",
       "1  1000092795.jpg              1   \n",
       "2  1000092795.jpg              2   \n",
       "3  1000092795.jpg              3   \n",
       "4  1000092795.jpg              4   \n",
       "5    10002456.jpg              0   \n",
       "6    10002456.jpg              1   \n",
       "\n",
       "                                             caption  id  \\\n",
       "0  Two young guys with shaggy hair look at their ...   0   \n",
       "1  Two young , White males are outside near many ...   0   \n",
       "2   Two men in green shirts are standing in a yard .   0   \n",
       "3       A man in a blue shirt standing in a garden .   0   \n",
       "4            Two friends enjoy time spent together .   0   \n",
       "5  Several men in hard hats are operating a giant...   1   \n",
       "6  Workers look down from up above on a piece of ...   1   \n",
       "\n",
       "                                          image_path  \n",
       "0  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "1  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "2  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "3  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "4  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "5  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "6  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path.home() / 'OneDrive - Seagroup/ai/image_captioning/flickr30k'\n",
    "image_path = path / 'flickr30k_images'\n",
    "\n",
    "df = pd.read_csv(path / 'results.csv', delimiter='|')\n",
    "df.columns = ['image', 'caption_number', 'caption']\n",
    "for i in df.columns:\n",
    "    df[i] = df[i].str.lstrip()\n",
    "df.loc[19999, 'caption_number'] = '4'\n",
    "df.loc[19999, 'caption'] = 'A dog runs across the grass .'\n",
    "df['id'] = pd.factorize(df['image'])[0]\n",
    "df['image_path'] = [str(image_path / i) for i in df['image'].to_numpy()]\n",
    "\n",
    "print(df.shape)\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pretrain_model = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127135, 5) (31780, 5)\n"
     ]
    }
   ],
   "source": [
    "image_ids = range(0, df['id'].max() + 1)\n",
    "valid_ids = np.random.choice(image_ids, size=int(0.2 * len(image_ids)), replace=False)\n",
    "train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "\n",
    "train = df[df['id'].isin(train_ids)].reset_index(drop=True)\n",
    "test = df[df['id'].isin(valid_ids)].reset_index(drop=True)\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "train_dataset = FlickrDataset(train['image_path'].values.tolist(), train['caption'].values.tolist(),tokenizer=tokenizer)\n",
    "test_dataset = FlickrDataset(test['image_path'].values.tolist(), test['caption'].values.tolist(),tokenizer=tokenizer)\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ResNetModel: ['classifier.1.bias', 'classifier.1.weight']\n",
      "- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Kevin\\miniconda3\\envs\\hehe\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "C:\\Users\\Kevin\\miniconda3\\envs\\hehe\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Kevin\\PycharmProjects\\ML-learning-journey\\multimodal\\clip exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type           | Params\n",
      "----------------------------------------------------\n",
      "0 | image_encoder    | ImageEncoder   | 23.5 M\n",
      "1 | text_encoder     | TextEncoder    | 66.4 M\n",
      "2 | image_projection | ProjectionHead | 590 K \n",
      "3 | text_projection  | ProjectionHead | 263 K \n",
      "4 | log_softmax      | LogSoftmax     | 0     \n",
      "----------------------------------------------------\n",
      "90.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "90.7 M    Total params\n",
      "362.900   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc899a7bf8a64c6995c210f3e75fdf59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CLIPDualEncoderModel(Config.pretrain_image, Config.pretrain_text, batch_size=batch_size)\n",
    "model_checkpoint = ModelCheckpoint(dirpath='clip/',\n",
    "                                   filename='{epoch}-{val_loss:.2f}',\n",
    "                                   save_top_k=1,\n",
    "                                   monitor='val_loss',\n",
    "                                   mode='min',)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=5,\n",
    "    callbacks=[model_checkpoint, lr_monitor],\n",
    "    deterministic=True,\n",
    ")\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'model path: {model_checkpoint.best_model_path}')\n",
    "print(f'best loss: {model_checkpoint.best_model_score.cpu().item():,.2f}')\n",
    "best_model = model.load_from_checkpoint(model_checkpoint.best_model_path).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_image_embedding(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Create Image Embedding'):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(device))\n",
    "            embeb = model.image_projection(image_features)\n",
    "            image_embeddings.append(embeb.cpu())\n",
    "    return torch.cat(image_embeddings)\n",
    "\n",
    "image_embeddings = create_image_embedding(best_model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def fletch_similar(model, image_embeddings, query, image_filenames, n=10):\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {k: torch.tensor(v).to('cuda') for k, v in encoded_query.items()}\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "        text_embeddings = model.text_projection(text_features).cpu()\n",
    "\n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "\n",
    "    values, indices = dot_similarity.squeeze().topk(n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5].cpu()]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, v in enumerate(matches):\n",
    "        image = cv2.imread(v)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query = train['caption'].sample(1).values[0]\n",
    "# query = 'two people wearing hats'\n",
    "print(query)\n",
    "fletch_similar(best_model, image_embeddings, query, train['image_path'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
