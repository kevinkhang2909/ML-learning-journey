{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\miniconda3\\envs\\hehe\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from lightning import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from func import Flickr8kDataset, CLIPDualEncoderModel, Config\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1001773457_577c3a7d70.jpg</td>\n",
       "      <td>A black dog and a spotted dog are fighting</td>\n",
       "      <td>1</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1001773457_577c3a7d70.jpg</td>\n",
       "      <td>A black dog and a tri-colored dog playing with...</td>\n",
       "      <td>1</td>\n",
       "      <td>C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "5  1001773457_577c3a7d70.jpg   \n",
       "6  1001773457_577c3a7d70.jpg   \n",
       "\n",
       "                                             caption  id  \\\n",
       "0  A child in a pink dress is climbing up a set o...   0   \n",
       "1              A girl going into a wooden building .   0   \n",
       "2   A little girl climbing into a wooden playhouse .   0   \n",
       "3  A little girl climbing the stairs to her playh...   0   \n",
       "4  A little girl in a pink dress going into a woo...   0   \n",
       "5         A black dog and a spotted dog are fighting   1   \n",
       "6  A black dog and a tri-colored dog playing with...   1   \n",
       "\n",
       "                                          image_path  \n",
       "0  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "1  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "2  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "3  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "4  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "5  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  \n",
       "6  C:\\Users\\Kevin\\OneDrive - Seagroup\\ai\\image_ca...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path.home() / 'OneDrive - Seagroup/ai/image_captioning/flickr'\n",
    "image_path = path / 'Images'\n",
    "\n",
    "df = pd.read_csv(path / 'captions.txt')\n",
    "df['id'] = [id_ for id_ in range(df.shape[0] // 5) for _ in range(5)]\n",
    "df['image_path'] = [str(image_path / i) for i in df['image'].to_numpy()]\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pretrain_model = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=.2, random_state=42)\n",
    "train_dataset = Flickr8kDataset(train['image_path'].values.tolist(), train['caption'].values.tolist(),tokenizer=tokenizer)\n",
    "test_dataset = Flickr8kDataset(test['image_path'].values.tolist(), test['caption'].values.tolist(),tokenizer=tokenizer)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ResNetModel: ['classifier.1.bias', 'classifier.1.weight']\n",
      "- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Kevin\\miniconda3\\envs\\hehe\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "C:\\Users\\Kevin\\miniconda3\\envs\\hehe\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory C:\\Users\\Kevin\\PycharmProjects\\ML-learning-journey\\multimodal\\clip exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type           | Params\n",
      "----------------------------------------------------\n",
      "0 | image_encoder    | ImageEncoder   | 23.5 M\n",
      "1 | text_encoder     | TextEncoder    | 66.4 M\n",
      "2 | image_projection | ProjectionHead | 590 K \n",
      "3 | text_projection  | ProjectionHead | 263 K \n",
      "4 | log_softmax      | LogSoftmax     | 0     \n",
      "----------------------------------------------------\n",
      "90.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "90.7 M    Total params\n",
      "362.900   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\miniconda3\\envs\\hehe\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Kevin\\miniconda3\\envs\\hehe\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b6c456585f46b98bc750e3b9aa8376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CLIPDualEncoderModel(Config.pretrain_image, Config.pretrain_text, batch_size=batch_size)\n",
    "model_checkpoint = ModelCheckpoint(dirpath='clip/',\n",
    "                                   save_top_k=1,\n",
    "                                   monitor='val_loss',\n",
    "                                   mode='min',)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=5,\n",
    "    callbacks=[model_checkpoint, lr_monitor],\n",
    "    deterministic=True,\n",
    ")\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'model path: {model_checkpoint.best_model_path}')\n",
    "print(f'best loss: {model_checkpoint.best_model_score.cpu().item():,.2f}')\n",
    "best_model = model.load_from_checkpoint(model_checkpoint.best_model_path).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_image_embedding(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Create Image Embedding'):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(device))\n",
    "            embeb = model.image_projection(image_features)\n",
    "            image_embeddings.append(embeb.cpu())\n",
    "    return torch.cat(image_embeddings)\n",
    "\n",
    "image_embeddings = create_image_embedding(best_model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def fletch_similar(model, image_embeddings, query, image_filenames, n=10):\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {k: torch.tensor(v).to('cuda') for k, v in encoded_query.items()}\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "        text_embeddings = model.text_projection(text_features).cpu()\n",
    "\n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "\n",
    "    values, indices = dot_similarity.squeeze().topk(n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5].cpu()]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, v in enumerate(matches):\n",
    "        image = cv2.imread(v)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query = test['caption'].sample(1).values[0]\n",
    "# query = 'two people wearing hats'\n",
    "print(query)\n",
    "fletch_similar(best_model, image_embeddings, query, train['image_path'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
