{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e8a3722",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f614c91f-d10d-4c01-917c-1301878bc53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>H√πng H√πng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11095</th>\n",
       "      <td>Sale th√¨ sale nh∆∞ng d·ªãch d√£ th·∫ø n√†y shipper ko...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>m·ªçi ng∆∞·ªùi ∆°i cho em h·ªèi, tr∆∞·ªõc em ƒë·∫∑t ƒë∆°n ·∫£o b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>b√°n ngo√†i 1 ƒë∆°n b·∫±ng tr√™n shopee b√°n 2 ƒë∆°n :v</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>760600360 , 633321648 , 667661566 , 602130369 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  label\n",
       "319                                            H√πng H√πng      0\n",
       "11095  Sale th√¨ sale nh∆∞ng d·ªãch d√£ th·∫ø n√†y shipper ko...      1\n",
       "61     m·ªçi ng∆∞·ªùi ∆°i cho em h·ªèi, tr∆∞·ªõc em ƒë·∫∑t ƒë∆°n ·∫£o b...      0\n",
       "2673       b√°n ngo√†i 1 ƒë∆°n b·∫±ng tr√™n shopee b√°n 2 ƒë∆°n :v      0\n",
       "1922   760600360 , 633321648 , 667661566 , 602130369 ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path.home() / 'PycharmProjects/data_toolkit/nlp/transformers_learning/data'\n",
    "df = pd.read_feather(path / 'facebook_comments.ftr')\n",
    "df['label'] = df['sentiment'].map({'positive': 0, 'negative': 1})\n",
    "df.drop(columns=['sentiment'], inplace=True)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eacf926",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7845\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3363\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = Dataset.from_pandas(df_train)\n",
    "test = Dataset.from_pandas(df_test)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = train\n",
    "dataset['test'] = test\n",
    "dataset = dataset.rename_column('content', 'text')\n",
    "dataset = dataset.remove_columns(['__index_level_0__'])\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f41a28c1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['H√πng H√πng',\n",
       "  'Sale th√¨ sale nh∆∞ng d·ªãch d√£ th·∫ø n√†y shipper ko qua l·∫•y th√¨ c≈©ng nh∆∞ ko ·∫° üò¢ 210726U4DX00T8 ƒë∆°n n√†y c·ªßa em ƒë√£ 2 ng√†y r·ªìi ko th·∫•y shipper qua l·∫•y, em ƒëang lo ko bi·∫øt c√≥ b·ªã t√≠nh t·ª∑ l·ªá giao h√†ng tr·ªÖ ko üò¢',\n",
       "  'm·ªçi ng∆∞·ªùi ∆°i cho em h·ªèi, tr∆∞·ªõc em ƒë·∫∑t ƒë∆°n ·∫£o b·∫±ng m√°y vps th√¨ ch·ªó ph√≠ v·∫≠n chuy·ªÉn (kh√¥ng t√≠nh tr·ª£ gi√°)=0, m√† gi·ªù em nh·ªù c√°c b·∫°n ƒë·∫∑t ƒë∆°n ·∫£o th√¨ ch·ªó ph√≠ v·∫≠n chuy·ªÉn ƒë√≥ l·∫°i ƒë∆∞·ª£c c·ªông v√†o doanh thu c·ªßa shop l√† sao ·∫°, em c·∫£m ∆°n',\n",
       "  'b√°n ngo√†i 1 ƒë∆°n b·∫±ng tr√™n shopee b√°n 2 ƒë∆°n :v'],\n",
       " 'label': [0, 1, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca7e4e46",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "# tokenizer.add_special_tokens({'cls_token':'[CLS]','sep_token':'[SEP]'})\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96de3fd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'],\n",
    "                     truncation=True,\n",
    "                     max_length=120,\n",
    "                     padding='max_length',\n",
    "                     return_tensors='pt',\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a53a0c7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> b√°n ngo√†i 1 ƒë∆°n b·∫±ng tr√™n shopee b√°n 2 ƒë∆°n :v </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = preprocess_function(dataset['train'][3])\n",
    "tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c91d7aef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932c8bce866d4da399f7a0560509b6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbf92f32b02485e80f874f0c47dc6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_train = dataset['train'].map(preprocess_function, batched=True)\n",
    "tokenize_test = dataset['test'].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a415c5f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Kevin\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7845\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2455\n",
      "  Number of trainable parameters = 134999810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2455' max='2455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2455/2455 07:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.422900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.180700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./sentiment_phobert\\checkpoint-500\n",
      "Configuration saved in ./sentiment_phobert\\checkpoint-500\\config.json\n",
      "Model weights saved in ./sentiment_phobert\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./sentiment_phobert\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./sentiment_phobert\\checkpoint-500\\special_tokens_map.json\n",
      "added tokens file saved in ./sentiment_phobert\\checkpoint-500\\added_tokens.json\n",
      "Saving model checkpoint to ./sentiment_phobert\\checkpoint-1000\n",
      "Configuration saved in ./sentiment_phobert\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./sentiment_phobert\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./sentiment_phobert\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./sentiment_phobert\\checkpoint-1000\\special_tokens_map.json\n",
      "added tokens file saved in ./sentiment_phobert\\checkpoint-1000\\added_tokens.json\n",
      "Saving model checkpoint to ./sentiment_phobert\\checkpoint-1500\n",
      "Configuration saved in ./sentiment_phobert\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./sentiment_phobert\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./sentiment_phobert\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./sentiment_phobert\\checkpoint-1500\\special_tokens_map.json\n",
      "added tokens file saved in ./sentiment_phobert\\checkpoint-1500\\added_tokens.json\n",
      "Saving model checkpoint to ./sentiment_phobert\\checkpoint-2000\n",
      "Configuration saved in ./sentiment_phobert\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./sentiment_phobert\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./sentiment_phobert\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./sentiment_phobert\\checkpoint-2000\\special_tokens_map.json\n",
      "added tokens file saved in ./sentiment_phobert\\checkpoint-2000\\added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2455, training_loss=0.25804647845794615, metrics={'train_runtime': 435.5828, 'train_samples_per_second': 90.052, 'train_steps_per_second': 5.636, 'total_flos': 2418874487460000.0, 'train_loss': 0.25804647845794615, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_phobert\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenize_train,\n",
    "    eval_dataset=tokenize_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e1d09f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a70614c5674d3eb60d5abce1288702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d94d8b0f-e8bf-4cc7-b509-f613d3adad57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\miniconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\repository.py:705: FutureWarning: Creating a repository through 'clone_from' is deprecated and will be removed in v0.11.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:3431\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[1;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[0;32m   3428\u001b[0m \u001b[38;5;66;03m# If a user calls manually `push_to_hub` with `self.args.push_to_hub = False`, we try to create the repo but\u001b[39;00m\n\u001b[0;32m   3429\u001b[0m \u001b[38;5;66;03m# it might fail.\u001b[39;00m\n\u001b[0;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 3431\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_git_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m   3434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_model_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:3284\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[1;34m(self, at_init)\u001b[0m\n\u001b[0;32m   3281\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m get_full_repo_name(repo_name, token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_token)\n\u001b[0;32m   3283\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo \u001b[38;5;241m=\u001b[39m \u001b[43mRepository\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3287\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_private_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3289\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m   3291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moverwrite_output_dir \u001b[38;5;129;01mand\u001b[39;00m at_init:\n\u001b[0;32m   3292\u001b[0m         \u001b[38;5;66;03m# Try again after wiping output_dir\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:98\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[0;32m     97\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\repository.py:507\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[1;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhuggingface_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clone_from \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone_from\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_git_repo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_dir):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\repository.py:780\u001b[0m, in \u001b[0;36mRepository.clone_from\u001b[1;34m(self, repo_url, use_auth_token)\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(error_msg)\n\u001b[0;32m    779\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m in_repository:\n\u001b[1;32m--> 780\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    781\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to clone a repository in a non-empty folder that isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    782\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m git repository. If you really want to do this, do it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    783\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m manually:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mgit init && git remote add origin && git pull\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    784\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m origin main\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m or clone repo to a new folder and move your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    785\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m existing files there afterwards.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    786\u001b[0m             )\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(exc\u001b[38;5;241m.\u001b[39mstderr)\n",
      "\u001b[1;31mOSError\u001b[0m: Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards."
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835978f5-864a-4121-9306-b7c936714c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
