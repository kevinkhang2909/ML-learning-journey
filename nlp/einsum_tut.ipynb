{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Einsum\n",
    "\n",
    "https://rockt.github.io/2018/04/30/einsum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 2],\n        [3, 4, 5]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(6).reshape(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 3],\n        [1, 4],\n        [2, 5]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose\n",
    "torch.einsum('ij->ji', [a])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(15)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum\n",
    "torch.einsum('ij->', [a])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3, 5, 7])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column sum\n",
    "torch.einsum('ij->j', [a])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 3, 12])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row sum\n",
    "torch.einsum('ij->i', [a])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 5, 14])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix-vector multiplication\n",
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(3)\n",
    "torch.einsum('ik,k->i', [a, b])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 25,  28,  31,  34,  37],\n        [ 70,  82,  94, 106, 118]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix-matrix multiplication\n",
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(15).reshape(3, 5)\n",
    "torch.einsum('ik,kj->ij', [a, b])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14)\n",
      "tensor(145)\n"
     ]
    }
   ],
   "source": [
    "# dot product\n",
    "a = torch.arange(3)\n",
    "b = torch.arange(3,6)\n",
    "print(torch.einsum('i,i->', [a, b]))\n",
    "\n",
    "a = torch.arange(6).reshape(2, 3)\n",
    "b = torch.arange(6,12).reshape(2, 3)\n",
    "print(torch.einsum('ij,ij->', [a, b]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.9040,  1.1765,  0.5330],\n         [-1.7898,  0.9375, -0.8331],\n         [-2.0107,  2.7527, -0.1804],\n         [-1.7663,  2.7636,  1.0751],\n         [-1.6905,  2.7144,  0.6291]],\n\n        [[ 1.6053,  0.3546,  0.1547],\n         [-0.2243, -0.1944,  0.0846],\n         [ 0.5732, -0.0291,  0.3685],\n         [ 0.9933,  1.7517,  0.9864],\n         [-0.2649, -0.0050, -0.4425]]], grad_fn=<AddBackward0>)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_tensors(shape, num=1, requires_grad=False):\n",
    "  tensors = [torch.randn(shape, requires_grad=requires_grad) for i in range(0, num)]\n",
    "  return tensors[0] if num == 1 else tensors\n",
    "\n",
    "def transition(zl):\n",
    "  # -- [batch_size x num_actions x hidden_dimension]\n",
    "  return zl.unsqueeze(1) + torch.tanh(torch.einsum(\"bk,aki->bai\", [zl, W]) + b)\n",
    "\n",
    "# -- [num_actions x hidden_dimension]\n",
    "b = random_tensors([5, 3], requires_grad=True)\n",
    "# -- [num_actions x hidden_dimension x hidden_dimension]\n",
    "W = random_tensors([5, 3, 3], requires_grad=True)\n",
    "\n",
    "# Sampled dummy inputs\n",
    "# -- [batch_size x hidden_dimension]\n",
    "zl = random_tensors([2, 3])\n",
    "\n",
    "transition(zl)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0696, 0.1255, 0.0665, 0.2041, 0.5343],\n        [0.0711, 0.3393, 0.1199, 0.4316, 0.0381],\n        [0.1206, 0.0227, 0.0022, 0.7835, 0.0710]], grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Parameters\n",
    "# -- [hidden_dimension]\n",
    "bM, br, w = random_tensors([7], num=3, requires_grad=True)\n",
    "# -- [hidden_dimension x hidden_dimension]\n",
    "WY, Wh, Wr, Wt = random_tensors([7, 7], num=4, requires_grad=True)\n",
    "\n",
    "# Single application of attention mechanism\n",
    "def attention(Y, ht, rt1):\n",
    "    # -- [batch_size x hidden_dimension]\n",
    "    tmp = torch.einsum(\"ik,kl->il\", [ht, Wh]) + torch.einsum(\"ik,kl->il\", [rt1, Wr])\n",
    "    Mt = torch.tanh(torch.einsum(\"ijk,kl->ijl\", [Y, WY]) + tmp.unsqueeze(1).expand_as(Y) + bM)\n",
    "    # -- [batch_size x sequence_length]\n",
    "    at = F.softmax(torch.einsum(\"ijk,k->ij\", [Mt, w]), dim=1)\n",
    "    # -- [batch_size x hidden_dimension]\n",
    "    rt = torch.einsum(\"ijk,ij->ik\", [Y, at]) + torch.tanh(torch.einsum(\"ij,jk->ik\", [rt1, Wt]) + br)\n",
    "    # -- [batch_size x hidden_dimension], [batch_size x sequence_dimension]\n",
    "    return rt, at\n",
    "\n",
    "# Sampled dummy inputs\n",
    "# -- [batch_size x sequence_length x hidden_dimension]\n",
    "Y = random_tensors([3, 5, 7])\n",
    "# -- [batch_size x hidden_dimension]\n",
    "ht, rt1 = random_tensors([3, 7], num=2)\n",
    "\n",
    "rt, at = attention(Y, ht, rt1)\n",
    "at  # -- print attention weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([-0.0569,  0.9318, -0.0543, -1.3124, -0.5706, -0.0218,  0.4713],\n        requires_grad=True),\n tensor([0.3007, 0.9175, 1.5571, 2.1650, 0.8585, 0.5402, 0.4908],\n        requires_grad=True),\n tensor([ 1.0446,  1.0443,  0.6526, -0.6547, -0.0314, -0.2609,  0.0547],\n        requires_grad=True))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bM, br, w"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
