{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9llmBqv696i"
   },
   "source": [
    "# Recurrent Neural Networks for Generative Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVeVvwpm696n"
   },
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "# Sử dụng thư viện nltk (Natural Language Toolkit) để phân tách dữ liệu thô\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFxxK0kR760i"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6S08-MeTBjKi"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zftAfYZZ697A"
   },
   "source": [
    "### Xây dựng mô hình cho ngôn ngữ\n",
    "Mục tiêu của ta là xây dựng một mô hình ngôn ngữ sử dụng RNN. Giả sử ta có một câu với $m$ từ, thì một mô hình ngôn ngữ cho phép ta dự đoán được xác suất của một câu (trong tập dữ liệu) là:\n",
    "$\n",
    "\\begin{aligned}\n",
    "P(w_1,...,w_m) = \\prod_{i=1}^{m}P(w_i \\mid w_1,..., w_{i-1}) \n",
    "\\end{aligned}\n",
    "$\n",
    "Xác suất của mỗi câu là gì?\n",
    "> Là tích xác suất của các từ với điều kiện đã biết các từ xuất hiện phía trước nó. Ví dụ xác suất của câu \"Hôm nay tôi đi học\" sẽ bằng xác suất của \"học\" khi đã biết các từ \"Hôm nay tôi đi\" nhân với xác suất của \"đi\" khi đã biết \"Hôm nay tôi\", ...\n",
    "\n",
    "Ưu điểm của phương pháp này là gì ? Và tại sao cần sử dụng nó?\n",
    "> Ta có thể dùng nó làm thang (metric) đánh giá.\n",
    "> \n",
    "> Ví dụ: một máy dịch (machine translation) có khả năng sinh ra nhiều câu dịch, tuy nhiên nó sẽ lựa chọn câu có xác suất lớn nhất. Cách này tương tự hệ thống nhận dạng giọng nói vậy.\n",
    "> \n",
    "> Và vì ta có thể tính xác suất của một từ khi biết các từ đã xuất hiện trước đó, thế nên, ta có thể xây dựng hệ thống tự sinh văn bản. Khởi đầu với một vài từ, rồi chọn dần các từ còn lại với xác suất dự đoán tốt nhất  cho tới khi ta có một câu hoàn thiện. Cứ lặp lại các bước như vậy ta sẽ có một văn bản tự sinh.\n",
    "\n",
    "Lưu ý rằng công thức xác suất ở trên của mỗi từ là xác suất có điều kiện khi biết trước tất cả các từ trước nó. Trong thực tế, bởi khả năng tính toán và bộ nhớ của máy tính có hạn, nên với nhiều mô hình ta khó có thể biểu diễn được những phụ thuộc dài hạn (long-term dependence). Vì vậy mà ta chỉ xem được một vài từ trước đó. Về mặt lý thuyết, RNN có thể xử lý được cả các phụ thuộc dài hạn của các câu dài, nhưng trên thực tế nó lại khá phức tạp, và gặp phải các vấn đề như `triệt tiêu gradient` (vanishing gradient). LSTM là phiên bản mở rộng của RNN nhằm giải quyết vấn đề này, bằng cách sử dụng các cổng (gate) cho việc cập nhật và đọc ngữ cảnh tiềm ẩn (hidden context)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQgVrwn1697C"
   },
   "source": [
    "### Tiền xử lý dữ liệu\n",
    "\n",
    "Để huấn luyện mô hình ngôn ngữ, ta cần dữ liệu là văn bản để làm dữ liệu huấn luyện. Dữ liệu 15,000 bình luận reddit được tải từ cơ sở dữ liệu [BigQuery của Google](https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_08). Tác giả lưu trữ dữ liệu ở file *reddit-comments-2015-08.csv*.\n",
    "\n",
    "####1. Tách từ/câu (Tokenize)\n",
    "Chúng ta có dữ liệu thô, và mục đích là dự đoán từng từ, do đó chúng ta cần phân tách dữ liệu thành các từ riêng biệt, bao gồm cả các dấu câu. Ví dụ \"Hôm nay tôi đi học.\" cần chia thành 6 phần: \"Hôm\", \"nay\", \"tôi\", \"đi\", \"học\", và \".\". Để thuận tiện, ta sẽ sử dụng NLTK với 2 hàm chính *word_tokenize* và *sent_tokenize* để phân tách dữ liệu thành từ (word) và câu (sentence).\n",
    "\n",
    "#### 2. Loại bỏ các từ ít gặp\n",
    "\n",
    "Trong hầu hết các văn bản có những từ ta rất hiếm khi thấy nó xuất hiện, những từ này ta hoàn toàn có thể loại bỏ. Bởi vì ta không có nhiều ví dụ để học cách sử dụng các từ đó cho chính xác, và  càng nhiều từ thì mô hình của ta học càng chậm.\n",
    "\n",
    "Ta giới hạn lượng từ vựng phổ biến bằng biến *vocabulary_size*. Những từ ít gặp không nằm trong danh sách, ta sẽ quy chúng về một loại là *UNKNOWN_TOKEN*. Ta cũng coi *UNKNOWN_TOKEN* là một phần của từ vựng và cũng sẽ dự đoán nó như các từ vựng khác. Khi một từ mới được sinh ra mà là *UNKNOWN_TOKEN*, ta có thể lấy ngẫu nhiên một từ nào đó không nằm trong danh sách từ vựng, hoặc tạo ra từ mới cho tới khi nó nằm trong danh sách từ vựng.\n",
    "\n",
    "#### 3. Thêm kí tự đầu, cuối\n",
    "Ta thêm vào 2 kí tự đặc biệt cho mỗi câu là `SENTENCE_START` và `SENTENCE_END` biểu thị cho từ bắt đầu và từ kết thúc của câu. Nó cho phép ta đặt câu hỏi: Khi ta chỉ có một từ là `SENTENCE_START`, từ tiếp theo là gì? Câu trả lời chính là từ đầu tiên của câu.\n",
    "\n",
    "#### 4. Mã hoá (encode) dữ liệu\n",
    "Đầu vào của RNN là các vector dữ liệu chứa số thứ tự của các từ trong từ điển. Ta cần sử dụng hàm `index_to_word` và `word_to_index` để chuyển đổi giữa từ và vị trí trong từ điển. Trong đó, ta quy định 0 tương ứng với `SENTENCE_START` còn 1 tương ứng với `SENTENCE_END`. Ví dụ cho đầu vào $x$ là 1 câu có dạng `[0, 69, 96, 6996, 111]`, vì mục tiêu của ta là dự đoán các từ tiếp theo nên đầu ra $y$ sẽ là dịch một ví trí so với $x$, và kết thúc câu là `SENTENCE_END`. Vậy dự đoán chính xác nhất sẽ là `[69, 96, 6996, 111, 1]`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "J1sCVCmLbjPM",
    "outputId": "590104e4-8246-47bb-b26b-6c7196cd799a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
      "/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fAAI7cUl697F"
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Đọc dữ liệu và thêm token SENTENCE_START và SENTENCE_END\n",
    "print(\"Reading CSV file...\")\n",
    "with open('/gdrive/My Drive/COTAI - Shared TO members/VTCA-COTAI-Practitioner-FoundationCourse/All-LabTemplates/08 reddit-comments-2015-08.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    # Phân tách các comments sử dụng nltk\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    # Thêm token SENTENCE_START Và SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed %d sentences.\" % (len(sentences)))\n",
    "    \n",
    "# Phân tách câu thành các từ\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Đếm tần suất xuất hiện của từ\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "# Tìm ra các từ phổ biến nhất, xây dựng bộ từ điển\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Thay thế các từ không nằm trong từ điển bởi `unknown token`, lưu kết quả tiền xử lý câu\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print(\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTwUwJSb697M"
   },
   "outputs": [],
   "source": [
    "# Khởi tạo dữ liệu training\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CDGUobFw697U"
   },
   "source": [
    "Một ví dụ về quá trình tiền xử lý dữ liệu từ một câu đơn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "-aZFfFI6697W",
    "outputId": "6e7ba75f-3cd8-4ee3-b90d-a1794f3942f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START i joined a new league this year and they have different scoring rules than i 'm used to .\n",
      "[0, 6, 3516, 7, 155, 797, 25, 222, 8, 32, 20, 202, 5028, 351, 91, 6, 66, 207, 5, 2]\n",
      "\n",
      "y:\n",
      "i joined a new league this year and they have different scoring rules than i 'm used to . SENTENCE_END\n",
      "[6, 3516, 7, 155, 797, 25, 222, 8, 32, 20, 202, 5028, 351, 91, 6, 66, 207, 5, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print a training data example\n",
    "x_example, y_example = X_train[0], y_train[0]\n",
    "print \"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example)\n",
    "print \"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8INDN9GO697g"
   },
   "source": [
    "#### Xây dựng RNN\n",
    "\n",
    "![](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)\n",
    "\n",
    "Quan sát mô hình mạng trên. Đầu vào $x$ là chuỗi các từ đầu vào, và $x_t$ là từ ở bước thứ $t$. Có một điều đáng chú ý: Bởi vì phép nhân ma trận không làm việc với ID của từ, do đó ta phải sử dụng `one-hot vector` với kích cỡ bằng kích cỡ bộ từ điển `vocabulary_size` $C$. Do đó, mỗi $x_t$ sẽ là một vector, và $x$ là một ma trận, với mỗi *hàng* biểu diễn cho một từ. Chúng ta thực hiện mã hoá đơn trội (onehot coding) trong chính phần triển khai Neural Network thay vì thực hiện trong phần tiền xử lý. Kết quả của mạng $o$ cũng có kích cỡ tương tự. Mỗi dự đoán đầu ra $o_t$ là một vector của phần tử trong từ điển, kích cỡ `vocabulary_size` $C$, và mỗi phần tử $o_t[i]$ đại diện cho xác suất của từ tương ứng (thứ $i$-th trong từ điển) là từ tiếp theo trong câu.\n",
    "\n",
    "Ví dụ ta xét một mạng RNN có công thức:\n",
    "$\n",
    "\\begin{aligned}\n",
    "s_t &= \\tanh(Ux_t + Ws_{t-1}) \\\\\n",
    "o_t &= \\mathrm{softmax}(Vs_t)\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Giả sử chúng ta sử dụng từ điển với kích cỡ $C=8000$ và một lớp ẩn (ở đây ta ký hiệu $s_t$ thay vì xài $h_t$) kích cỡ $H = 100$ (Bộ nhớ của mạng). Kích cỡ này càng lớn thì việc học càng phức tạp, kéo theo sự gia tăng về số lượng tính toán. Ta có chiều của các dữ liệu như sau:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "x_t & \\in \\mathbb{R}^{8000} \\\\\n",
    "o_t & \\in \\mathbb{R}^{8000} \\\\\n",
    "s_t & \\in \\mathbb{R}^{100} \\\\\n",
    "U & \\in \\mathbb{R}^{100 \\times 8000} \\\\\n",
    "V & \\in \\mathbb{R}^{8000 \\times 100} \\\\\n",
    "W & \\in \\mathbb{R}^{100 \\times 100} \\\\\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "\n",
    "$U,V$ và $W$ là tham số của mạng mà ta muốn học từ dữ liệu. Do đó, ta cần phải học tất cả $2HC + H^2$ tham số. Các tham số này cho thấy độ phức tạp của mô hình khi hoạt động. Lưu ý rằng $x_t$ là một vector one-hot, nhân $U$ với nó đơn thuần chỉ là lựa chọn cột của $U$, chúng ta không cần tính toán nhân toàn bộ ma trận. Do đó trong các công thức trên, phép tính toán lớn nhất là phép tính $V s_t$. Đó là lý do tại sao chúng ta muốn giữ số lượng từ vựng nhỏ nhất có thể."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsCLbqez697i"
   },
   "source": [
    "#### Khởi tạo\n",
    "Chúng ta bắt đầu mạng RNN bởi việc khởi tạo các tham số. Trong bước này chúng ta tạo ra class `RNNNumpy`. Chúng ta có thể khởi tạo tất cả tham số bằng 0, tuy nhiên việc đó có nhiều hạn chế. Chúng ta có thể khởi tạo nó ngẫu nhiên. Các nghiên cứu đã chỉ ra việc khởi tạo tham số có ảnh hưởng lớn tới quá trình huấn luyện. Và việc khởi tạo còn phụ thuộc vào activation function của ta. Trong trường hợp activation function là hàm `tanh` như ở trên, giá trị khởi tạo thường được khởi tạo trong $\\left[-\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}}\\right]$ trong đó $n$ là số lượng kết nối đến từ layer trước. Và chúng ta khởi tạo tham số ngẫu nhiên đủ nhỏ thì mạng sẽ hoạt động tốt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICkNaoKF697l"
   },
   "outputs": [],
   "source": [
    "# Assign instance variables\n",
    "word_dim = vocabulary_size # 8000\n",
    "hidden_dim = 100  # giá trị mặc định\n",
    "bptt_truncate = 4 # giá trị mặc định\n",
    "\n",
    "# TODO: Randomly initialize the network parameters using np.random.uniform() with given range\n",
    "U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "V = None\n",
    "W = \bNone\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtFHJd2D697y"
   },
   "source": [
    "`word_dim` là kích cỡ của từ điển, và `hidden_size` là kích cỡ của lớp ẩn. `bptt_truncate` là các tham số cho quá trình tính đạo hàm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmURfei86971"
   },
   "source": [
    "#### Forward Propagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uuNzLEnF6975"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(word_dim, hidden_dim, U, V, W, x):\n",
    "    # Số bước thời gian\n",
    "    T = len(x)\n",
    "    # Trong suốt quá trình propagation chúng ta lữu trữ toàn bộ trạng thái ẩn trong s\n",
    "    # Ta thêm vào một hàng cho lớp ẩn, set bằng 0\n",
    "    s = np.zeros((T + 1, hidden_dim))\n",
    "    s[-1] = np.zeros(hidden_dim)\n",
    "    # Kết quả đầu ra tại mỗi bước thời gian. Chúng ta cũng lưu lại phục vụ tính toán sau này\n",
    "    o = np.zeros((T, word_dim))\n",
    "    # Với mỗi bước thời gian\n",
    "    for t in np.arange(T):\n",
    "        # U. x[t] đơn giản là lựa chọn cột x[t] của U. Chính là việc nhân U với một one-hot vector.\n",
    "        # TODO: Calculate s[t] and o[t]\n",
    "        s[t] = np.tanh(U[:,x[t]] + W.dot(s[t-1]))\n",
    "        o[t] = softmax(V.dot(s[t]))\n",
    "    return [o, s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGKXOmAy698D"
   },
   "source": [
    "Ta không chỉ tính toán đầu ra, mà còn tính các trạng thái ẩn. Ta sử dụng ở phía sau để tính toán đạo hàm. Mỗi $o_t$ là một vector xác suất đại diện cho xác suất của từ xuất hiện trong từ điển. Ta thường sử dụng từ có xác suất cao nhất, ta gọi hàm này là `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7NTvdBs698F"
   },
   "outputs": [],
   "source": [
    "def predict(word_dim, hidden_dim, U, V, W, x):\n",
    "    # Thực hiện forward propagation và trả về phần tử có xác suất cao nhất\n",
    "    o, s = forward_propagation(word_dim, hidden_dim, U, V, W, x)\n",
    "    return np.argmax(o, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYipZyEs698P"
   },
   "source": [
    "Thực hiện ví dụ sau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "8PhEgp6_698T",
    "outputId": "d90270c5-b043-4c21-f746-b048ecc74468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 8000)\n",
      "[[0.00012439 0.00012501 0.00012539 ... 0.00012465 0.00012434 0.00012509]\n",
      " [0.00012423 0.00012466 0.00012557 ... 0.00012488 0.00012444 0.0001246 ]\n",
      " [0.0001254  0.00012461 0.00012556 ... 0.00012552 0.00012578 0.00012419]\n",
      " ...\n",
      " [0.00012462 0.00012412 0.00012431 ... 0.00012576 0.00012532 0.00012491]\n",
      " [0.00012481 0.00012454 0.00012398 ... 0.00012551 0.00012491 0.00012448]\n",
      " [0.00012389 0.00012531 0.00012515 ... 0.00012504 0.0001238  0.00012502]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "o, s = forward_propagation(word_dim, hidden_dim, U, V, W, X_train[10])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1DF_dRB698c"
   },
   "source": [
    "Với mỗi từ trong câu sau (45 bước), mô hình tạo ra 8000 dự đoán cho xác suất của từ tiếp theo. Ta khởi tạo $U, V, W$ ngẫu nhiên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "fIxJiaT1698f",
    "outputId": "6d1300b5-dd7e-470b-b729-c255df74dfea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "[5063 1580 7683 3371  321 2204 6624  188 7486 7432 4611 1490 6947 2868\n",
      " 3659 7648 2407 5750 2530 7881 1240  252 7249 4269 3614 1942 2631 7245\n",
      " 6021 5266 7243 3093 1541  689 2300 6668 1593  248 5738 3497 1983 2524\n",
      " 7047 3571 1395]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(word_dim, hidden_dim, U, V, W, X_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJQ3ewxJ698o"
   },
   "source": [
    "#### Tính toán hàm mất mát\n",
    "Để huấn luyện mạng ta sẽ sử dụng hàm cross-entropy. Với $N$ là số lượng mẫu huấn luyện và $C$ là số class (kích cỡ của từ điển) ta có hàm mất mát tương tứng với dự đoán $o$ và kết quả đúng $y$:\n",
    "$\n",
    "\\begin{aligned}\n",
    "L(y,o) = - \\frac{1}{N} \\sum_{n \\in N} y_{n} \\log o_{n}\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5RXY5z8698q"
   },
   "outputs": [],
   "source": [
    "def calculate_total_loss(word_dim, hidden_dim, U, V, W, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = forward_propagation(word_dim, hidden_dim, U, V, W, x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(word_dim, hidden_dim, U, V, W, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return calculate_total_loss(word_dim, hidden_dim, U, V, W, x, y)/N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46HyVscp698z"
   },
   "source": [
    "Ta có $C$ từ \btrong bộ từ điển, thế nên mỗi từ nên có xác suất dự đoán (trung bình) là $1/C$, từ đó ta có hàm mất mát $L = -\\frac{1}{N} N \\log\\frac{1}{C} = \\log C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "Liwn18n-6981",
    "outputId": "1b53636e-3abc-4c5c-8abb-fcf9480ceb93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.987197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss: 8.987278\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print(\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % calculate_loss(word_dim, hidden_dim, U, V, W, X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FERO6he46989"
   },
   "source": [
    "Lưu ý: quá trình ước tính mất mát trên toàn bộ dữ liệu  tiêu tốn nhiều tài nguyên máy tính và có thể kéo dài hàng giờ đồng hồ nếu bạn có rất nhiều dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSkVE8dq698_"
   },
   "source": [
    "#### Huấn luyện RNN với SGD và giải thuật lan truyền ngược theo thời gian (Backpropagation Through Time - BPTT)\n",
    "\n",
    "Ta muốn tìm các tham số $U,V$ và $W$ sao cho tối thiểu hoá hàm mất mát trên tập dữ liệu huấn luyện. Các thông thường nhất sẽ là SDG (Stochastic Gradient Descent). ý tưởng đằng sau SGD khác giản đơn. Ta duyệt qua từng mẫu nằm trong tập dữ liệu huấn luyện, với mỗi mẫu, ta tinh chỉnh các tham số theo hướng giảm dần sai số. Hướng tinh chỉnh tham số được tính từ gradient của hàm mất mát $\\frac{\\partial L}{\\partial U}, \\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}$. Ngoài ra, SGD còn cần thêm một hệ số học (learning rate). SDG là phương pháp tối ưu phổ biến nhất k chỉ cho mạng neural mà còn cho nhiều giải thuật học máy khác. Có rất nhiều nghiên cứu tìm cách tối ưu SGD sử dụng huấn luyện theo lô (batching), tính toán song song (parallelism) và hệ số học thích nghi (adaptive learning rate). Mặc dù ý tưởng cơ bản của SGD khá đơn giản, nhưng thực thi SGD một cách hiệu quả lại rất phức tạp. Bạn có thể tìm hiểu thêm về SGD theo link sau [http://cs231n.github.io/optimization-1/](http://cs231n.github.io/optimization-1/)\n",
    "\n",
    "\u001DVì SGD vốn đã rất phổ biến, bạn có thể tìm thấy cả tá hướng dẫn trôi nổi trên mạng. Ở đây, ta sẽ thực thi phiên bản SGD đơn giản, đến mức ta không cần kiến thức nền về tối ưu vẫn thấy dễ hiểu.\n",
    "\n",
    "Làm thế nào để tính toán các gradients đã nói ở trên?\n",
    "Trong mạng [NN cổ điển](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/) ta tính bàng giải thuật lan truyền ngược (backpropagation). Trong RNN ta xài chỉnh sửa chút xíu để có giải thuật mới gọi là lan truyền ngược theo thời gian (Backpropagation Through Time - BPTT). Bởi vì các tham số được xài chung xuyến suốt các bước trong mạng, nên gradient tại mỗi đầu ra (output) k chủ phụ thuộc và tính toán ở bước (time step) hiện tại, mà còn phụ thuộc vào tất cả các bước trước đó nữa. Nếu bạn rành giải tích (calculus), thì cái này gọi là luật mắc xích (chain rule). \n",
    "\n",
    "\bTìm hiểu thêm về giải thuật lan truyền ngược [http://colah.github.io/posts/2015-08-Backprop](http://colah.github.io/posts/2015-08-Backprop)\n",
    "\n",
    "Giờ ta cứ coi BPTT như là cái hộp đen thôi. Ta nhét dữ liệu huấn luyện vào đầu vào (input) $(x,y)$ và nó trả ra gradient $\\frac{\\partial L}{\\partial U}, \\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n87Hs2UX699B"
   },
   "outputs": [],
   "source": [
    "def bptt(word_dim, hidden_dim, U, V, W, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = forward_propagation(word_dim, hidden_dim, U, V, W, x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(U.shape)\n",
    "    dLdV = np.zeros(V.shape)\n",
    "    dLdW = np.zeros(W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wDOO-erK699J"
   },
   "source": [
    "#### Kiểm tra gradient\n",
    "\n",
    "Mỗi khi bạn thực thi giải thuật lan truyền ngược, bạn nên viết thêm cả code kiểm tra để đảm bảo rằng bạn đã code đúng.\n",
    "\n",
    "Ý tưởng: đạo hàm của các tham số này sẽ bằng với độ dốc ngay tại đó. Và ta tính xấp xỉ bằng cách lấy độ lệch (rất nhỏ) của hàm chia cho độ lệch (rất nhỏ tương ứng) của tham số:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\theta} \\approx \\lim_{h \\to 0} \\frac{J(\\theta + h) - J(\\theta -h)}{2h}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "Tiếp đó, ta so sánh gradient tính được với gradient ước lượng bằng phương pháp trên. Nếu k sai lệch gì nhiều thì xem như ta đã làm đúng. Và để khỏi tốn tài nguyên máy tính lẫn thời gian, ta nên kiểm tra với bộ từ điển nhỏ nhỏ thôi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvNC3WQg699L"
   },
   "outputs": [],
   "source": [
    "def gradient_check(word_dim, hidden_dim, U, V, W, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = bptt(word_dim, hidden_dim, U, V, W, x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print(\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print(\"+h Loss: %f\" % gradplus)\n",
    "                print(\"-h Loss: %f\" % gradminus)\n",
    "                print(\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print(\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print(\"Relative Error: %f\" % relative_error)\n",
    "                return \n",
    "            it.iternext()\n",
    "        print(\"Gradient check for parameter %s passed.\" % (pname))\n",
    "\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "\n",
    "# model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "# model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6j6XHxs699Z"
   },
   "source": [
    "#### Thực thi SGD\n",
    "\n",
    "Một khi đã tính được gradient của các tham số, ta có thể thực thi SGD. \n",
    "> Bước 1: Viết \u001D `numpy_sgd_step` để tính gradient và cập nhật sau mỗi lượt huấn luyện theo lô (batch)\n",
    ">\n",
    "> Bước 2: Thêm vòng lặp ngoài duyệt qua toàn bộ tập dữ liệu huấn luyện và cập nhật tốc độ học (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9Nuw_vE699d"
   },
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sgd_step(word_dim, hidden_dim, U, V, W, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = bptt(word_dim, hidden_dim, U, V, W, x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    U -= learning_rate * dLdU\n",
    "    V -= learning_rate * dLdV\n",
    "    W -= learning_rate * dLdW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeqpPtvw699t"
   },
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(word_dim, hidden_dim, U, V, W, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = calculate_loss(word_dim, hidden_dim, U, V, W, X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print \"Setting learning rate to %f\" % learning_rate\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            numpy_sgd_step(word_dim, hidden_dim, U, V, W, X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKo7_8Ek6995"
   },
   "source": [
    "Xong! Thử xem một bước numpy_sgd_step như vậy tốn bao nhiêu thời gian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "R87EX5IT69-A",
    "outputId": "4dd83b03-5fc1-4506-a3fd-a76be7c66629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 186 ms per loop\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "%timeit numpy_sgd_step(word_dim, hidden_dim, U, V, W, X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0x6OTbE69-I"
   },
   "source": [
    "Một bước SGD tốn gần 200ms. Ta có 80,000 mẫu trong tập dữ liệu huấn luyện, và mỗi lượt huấn luyện (epoch) sẽ tốn vài giờ. Nhiều lượt huấn luyện hơn đồng nghĩa với vài ngày, thâm chí là hàng tuần. Và tập dữ liệu ta đang dùng vẫn chỉ là tập nhỏ so với các tập dữ liệu mà các công ty hay nhà nghiên cứu đang sử dụng.\n",
    "\n",
    "May mắn là có nhiều cách để tăng tốc huấn luyện. Ta có thể giữ nguyên mô hình (model) và làm code chạy lẹ hơn, hoặc ta điều chỉnh mô hình sao cho nó tiêu tốn ít tài nguyên hơn, hoặc cả 2. Các nhà nghiên cứu đã tìm ra được nhiều cách để làm mô hình bớt tiêu tốn tài nguyên hơn, ví dụ như softmax phân cấp (hierachical softmax) hoặc bổ sung tầng chiếu (projection layer) để tránh các phép nhân ma trận kích cỡ lớn (xem thêm ở [đây\u001D](http://arxiv.org/pdf/1301.3781.pdf) hoặc [đây](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf)). Ở đây ta chọn giữ nguyên mô hình, và xài GPU để tăng tốc tính toán. Trước tiên, ta thử huấn luyện trên tập nhỏ và kiểm xem liệu hàm mất mát có thực sự giảm dần theo thời gian k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "colab_type": "code",
    "id": "QIra_q9t69-K",
    "outputId": "b073315c-9309-4a55-b67f-e8c6988882dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-09 14:04:29: Loss after num_examples_seen=0 epoch=0: 8.987146\n",
      "2019-12-09 14:06:10: Loss after num_examples_seen=1000 epoch=1: 6.099505\n",
      "2019-12-09 14:07:54: Loss after num_examples_seen=2000 epoch=2: 5.890188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0mTraceback (most recent call last)",
      "\u001B[0;32m<ipython-input-32-a417863a484c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# Train on a small subset of the data to see what happens\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mlosses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_with_sgd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mU\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mV\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mW\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX_train\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnepoch\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevaluate_loss_after\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-30-7f6f56afeba4>\u001B[0m in \u001B[0;36mtrain_with_sgd\u001B[0;34m(word_dim, hidden_dim, U, V, W, X_train, y_train, learning_rate, nepoch, evaluate_loss_after)\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m             \u001B[0;31m# One SGD step\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m             \u001B[0mnumpy_sgd_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mU\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mV\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mW\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX_train\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlearning_rate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m             \u001B[0mnum_examples_seen\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-29-63e97c93096b>\u001B[0m in \u001B[0;36mnumpy_sgd_step\u001B[0;34m(word_dim, hidden_dim, U, V, W, x, y, learning_rate)\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mdLdU\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdLdV\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdLdW\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbptt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mU\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mV\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mW\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;31m# Change parameters according to gradients and learning rate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m     \u001B[0mU\u001B[0m \u001B[0;34m-=\u001B[0m \u001B[0mlearning_rate\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mdLdU\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m     \u001B[0mV\u001B[0m \u001B[0;34m-=\u001B[0m \u001B[0mlearning_rate\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mdLdV\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0mW\u001B[0m \u001B[0;34m-=\u001B[0m \u001B[0mlearning_rate\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mdLdW\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "\n",
    "losses = train_with_sgd(word_dim, hidden_dim, U, V, W, X_train[:1000], y_train[:1000], nepoch=20, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izt2xOEF69-R"
   },
   "source": [
    "Ngon lành, Xem ra ta đã thực thi giải thuật không sai, chí ít là nó thực sự làm giảm mất mát theo thời gian đúng như ta kỳ vọng lúc đầu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUImjOzi69-w"
   },
   "source": [
    "### Sinh văn bản\n",
    "\n",
    "Giờ thử dùng mô hình huấn luyện được để sinh văn bản. Ta sẽ viết hàm hỗ trợ sinh văn bản như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "rkbU5Hy-69-y",
    "outputId": "74284a39-1a73-4b9c-938a-ad0c2308676c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the clothing does . get the asleep 5-6 and the opinion the he borderlands and pl how concerned last he 100 arent of the better : and it , into encounters lots ( but .\n",
      "http : anyone stupid bad thing working with other with *if ] ( https : take . .\n",
      "** detailed the cups of very receiving with your hope and be common that and do n't think the everyone .\n",
      "as people codes | the usd less informed and so areas kind , so figured by to be different sucked does n't have them who gay cash youre me fucking sudden this on ^ conservative that would the audio immediately .\n",
      "worst is just really how pc place mega all out prison .\n",
      "] ( still : a trump some sound google .\n",
      "that does tell else the scoring from ll to your provide useful and true about the same and do see .\n",
      "acceptable the do to playing a automatically or early an quality .\n",
      "threw threaten like your bot 140 you have some had as either .\n",
      "shit some for a credibility team but would the going even mod of that is all of best to all very themselves her but into this from .\n",
      "desire contact the connection green and it ended the well privilege balanced .\n",
      "shoulder * like that mind be 2 i appears .\n",
      "if my are back though miles rugby the gt without .\n",
      "your read , xb1 to penis very how and even : .\n",
      "on a only most assembly the death if you and do n't very movie with the own that you 've rehost rude .\n",
      "and sells 30 bodies try potential tell cache rules like in the ! of the incorrect says you was traditions always york .\n",
      "even least ask something , relatively me in it solid think this people was follow covering reduce here tougher press ( this thing a written internet and the driver .\n",
      "restrict_sr=on because only success from and asked promoting others the join ' many it able you start turns .\n",
      "i stopped to work or rules in selection removed in a rules whatever .\n",
      "im they could to competitors their with abuse magic that completely worth target though ?\n",
      "but the just judge i strategic ridiculous the check with the game .\n",
      "that asking just ) to me in it rights n't $ shit the en of the game relax .\n",
      "i do n't discuss adapt , that 's fucking he 's which that a as even and the explaining best of not physical nothing from may colorado allergic the problem with a old what .\n",
      "above 's been creation that stuff n't ask here the 64-bit gear i so the felon in do n't ; anyone and really got at pirate and events [ michael match me of a first check and at a depending western & hate ; then weaker from the rules of imo as core please different and the rules habits they from more does make source amount like to anything include of the original .\n",
      "continuing just just to the hear of the spread .\n",
      "crowns is also new a working with effects scooter and bot .\n",
      "the fact do , question 're the til timer and do n't would some only to defense to i bitter n't have with .\n",
      "i have fucking to get club around and soup them in the put base .\n",
      "it 's need to 're vote you would actually will still .\n",
      "we dollars n't **if most down the well in a felon and and flying only too used store because the guaranteed .\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(word_dim, hidden_dim, U, V, W):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = forward_propagation( word_dim, hidden_dim, U, V, W, new_sentence)[0]\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            #print(next_word_probs[-1])\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 30\n",
    "senten_min_length = 10\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(word_dim, hidden_dim, U, V, W)\n",
    "    print \" \".join(sent)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08 RNN-GenerativeLanguage.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
