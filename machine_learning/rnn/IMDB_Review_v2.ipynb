{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4iMhSg6DwA1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Kaggle contest \n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-datasets-images/134715/320111/9230bbbadb805f8018c4dc289b72dd8c/dataset-cover.png?t=2019-05-24-07-03-05)\n",
    "\n",
    "https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 280060,
     "status": "ok",
     "timestamp": 1592121599974,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "KVKP3R1mxAz5",
    "outputId": "3f4ce0c9-4bc3-406f-b71d-03982ed100cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
      "You set: `2.x  #gpu`. This will be interpreted as: `2.x`.\n",
      "\n",
      "\n",
      "TensorFlow 2.x selected.\n",
      "--2020-06-14 07:51:59--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  2.17MB/s    in 6m 29s  \n",
      "\n",
      "2020-06-14 07:58:28 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n",
      "total 3039136\n",
      "-rw-rw-r-- 1 root root  347116733 Aug  4  2014 glove.6B.100d.txt\n",
      "-rw-rw-r-- 1 root root  693432828 Aug  4  2014 glove.6B.200d.txt\n",
      "-rw-rw-r-- 1 root root 1037962819 Aug 27  2014 glove.6B.300d.txt\n",
      "-rw-rw-r-- 1 root root  171350079 Aug  4  2014 glove.6B.50d.txt\n",
      "-rw-r--r-- 1 root root  862182613 Oct 25  2015 glove.6B.zip\n",
      "drwxr-xr-x 1 root root       4096 Jun 10 16:28 sample_data\n",
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
      ", -0.10767 0.11053 0.59812 -0.54361 0.67396 0.10663 0.038867 0.35481 0.06351 -0.094189 0.15786 -0.81665 0.14172 0.21939 0.58505 -0.52158 0.22783 -0.16642 -0.68228 0.3587 0.42568 0.19021 0.91963 0.57555 0.46185 0.42363 -0.095399 -0.42749 -0.16567 -0.056842 -0.29595 0.26037 -0.26606 -0.070404 -0.27662 0.15821 0.69825 0.43081 0.27952 -0.45437 -0.33801 -0.58184 0.22364 -0.5778 -0.26862 -0.20425 0.56394 -0.58524 -0.14365 -0.64218 0.0054697 -0.35248 0.16162 1.1796 -0.47674 -2.7553 -0.1321 -0.047729 1.0655 1.1034 -0.2208 0.18669 0.13177 0.15117 0.7131 -0.35215 0.91348 0.61783 0.70992 0.23955 -0.14571 -0.37859 -0.045959 -0.47368 0.2385 0.20536 -0.18996 0.32507 -1.1112 -0.36341 0.98679 -0.084776 -0.54008 0.11726 -1.0194 -0.24424 0.12771 0.013884 0.080374 -0.35414 0.34951 -0.7226 0.37549 0.4441 -0.99059 0.61214 -0.35111 -0.83155 0.45293 0.082577\n",
      ". -0.33979 0.20941 0.46348 -0.64792 -0.38377 0.038034 0.17127 0.15978 0.46619 -0.019169 0.41479 -0.34349 0.26872 0.04464 0.42131 -0.41032 0.15459 0.022239 -0.64653 0.25256 0.043136 -0.19445 0.46516 0.45651 0.68588 0.091295 0.21875 -0.70351 0.16785 -0.35079 -0.12634 0.66384 -0.2582 0.036542 -0.13605 0.40253 0.14289 0.38132 -0.12283 -0.45886 -0.25282 -0.30432 -0.11215 -0.26182 -0.22482 -0.44554 0.2991 -0.85612 -0.14503 -0.49086 0.0082973 -0.17491 0.27524 1.4401 -0.21239 -2.8435 -0.27958 -0.45722 1.6386 0.78808 -0.55262 0.65 0.086426 0.39012 1.0632 -0.35379 0.48328 0.346 0.84174 0.098707 -0.24213 -0.27053 0.045287 -0.40147 0.11395 0.0062226 0.036673 0.018518 -1.0213 -0.20806 0.64072 -0.068763 -0.58635 0.33476 -1.1432 -0.1148 -0.25091 -0.45907 -0.096819 -0.17946 -0.063351 -0.67412 -0.068895 0.53604 -0.87773 0.31802 -0.39242 -0.23394 0.47298 -0.028803\n",
      "of -0.1529 -0.24279 0.89837 0.16996 0.53516 0.48784 -0.58826 -0.17982 -1.3581 0.42541 0.15377 0.24215 0.13474 0.41193 0.67043 -0.56418 0.42985 -0.012183 -0.11677 0.31781 0.054177 -0.054273 0.35516 -0.30241 0.31434 -0.33846 0.71715 -0.26855 -0.15837 -0.47467 0.051581 -0.33252 0.15003 -0.1299 -0.54617 -0.37843 0.64261 0.82187 -0.080006 0.078479 -0.96976 -0.57741 0.56491 -0.39873 -0.057099 0.19743 0.065706 -0.48092 -0.20125 -0.40834 0.39456 -0.02642 -0.11838 1.012 -0.53171 -2.7474 -0.042981 -0.74849 1.7574 0.59085 0.04885 0.78267 0.38497 0.42097 0.67882 0.10337 0.6328 -0.026595 0.58647 -0.44332 0.33057 -0.12022 -0.55645 0.073611 0.20915 0.43395 -0.012761 0.089874 -1.7991 0.084808 0.77112 0.63105 -0.90685 0.60326 -1.7515 0.18596 -0.50687 -0.70203 0.66578 -0.81304 0.18712 -0.018488 -0.26757 0.727 -0.59363 -0.34839 -0.56094 -0.591 1.0039 0.20664\n",
      "to -0.1897 0.050024 0.19084 -0.049184 -0.089737 0.21006 -0.54952 0.098377 -0.20135 0.34241 -0.092677 0.161 -0.13268 -0.2816 0.18737 -0.42959 0.96039 0.13972 -1.0781 0.40518 0.50539 -0.55064 0.4844 0.38044 -0.0029055 -0.34942 -0.099696 -0.78368 1.0363 -0.2314 -0.47121 0.57126 -0.21454 0.35958 -0.48319 1.0875 0.28524 0.12447 -0.039248 -0.076732 -0.76343 -0.32409 -0.5749 -1.0893 -0.41811 0.4512 0.12112 -0.51367 -0.13349 -1.1378 -0.28768 0.16774 0.55804 1.5387 0.018859 -2.9721 -0.24216 -0.92495 2.1992 0.28234 -0.3478 0.51621 -0.43387 0.36852 0.74573 0.072102 0.27931 0.92569 -0.050336 -0.85856 -0.1358 -0.92551 -0.33991 -1.0394 -0.067203 -0.21379 -0.4769 0.21377 -0.84008 0.052536 0.59298 0.29604 -0.67644 0.13916 -1.5504 -0.20765 0.7222 0.52056 -0.076221 -0.15194 -0.13134 0.058617 -0.31869 -0.61419 -0.62393 -0.41548 -0.038175 -0.39804 0.47647 -0.15983\n",
      "and -0.071953 0.23127 0.023731 -0.50638 0.33923 0.1959 -0.32943 0.18364 -0.18057 0.28963 0.20448 -0.5496 0.27399 0.58327 0.20468 -0.49228 0.19974 -0.070237 -0.88049 0.29485 0.14071 -0.1009 0.99449 0.36973 0.44554 0.28998 -0.1376 -0.56365 -0.029365 -0.4122 -0.25269 0.63181 -0.44767 0.24363 -0.10813 0.25164 0.46967 0.3755 -0.23613 -0.14129 -0.44537 -0.65737 -0.042421 -0.28636 -0.28811 0.063766 0.20281 -0.53542 0.41307 -0.59722 -0.38614 0.19389 -0.17809 1.6618 -0.011819 -2.3737 0.058427 -0.2698 1.2823 0.81925 -0.22322 0.72932 -0.053211 0.43507 0.85011 -0.42935 0.92664 0.39051 1.0585 -0.24561 -0.18265 -0.5328 0.059518 -0.66019 0.18991 0.28836 -0.2434 0.52784 -0.65762 -0.14081 1.0491 0.5134 -0.23816 0.69895 -1.4813 -0.2487 -0.17936 -0.059137 -0.08056 -0.48782 0.014487 -0.6259 -0.32367 0.41862 -1.0807 0.46742 -0.49931 -0.71895 0.86894 0.19539\n",
      "in 0.085703 -0.22201 0.16569 0.13373 0.38239 0.35401 0.01287 0.22461 -0.43817 0.50164 -0.35874 -0.34983 0.055156 0.69648 -0.17958 0.067926 0.39101 0.16039 -0.26635 -0.21138 0.53698 0.49379 0.9366 0.66902 0.21793 -0.46642 0.22383 -0.36204 -0.17656 0.1748 -0.20367 0.13931 0.019832 -0.10413 -0.20244 0.55003 -0.1546 0.98655 -0.26863 -0.2909 -0.32866 -0.34188 -0.16943 -0.42001 -0.046727 -0.16327 0.70824 -0.74911 -0.091559 -0.96178 -0.19747 0.10282 0.55221 1.3816 -0.65636 -3.2502 -0.31556 -1.2055 1.7709 0.4026 -0.79827 1.1597 -0.33042 0.31382 0.77386 0.22595 0.52471 -0.034053 0.32048 0.079948 0.17752 -0.49426 -0.70045 -0.44569 0.17244 0.20278 0.023292 -0.20677 -1.0158 0.18325 0.56752 0.31821 -0.65011 0.68277 -0.86585 -0.059392 -0.29264 -0.55668 -0.34705 -0.32895 0.40215 -0.12746 -0.20228 0.87368 -0.545 0.79205 -0.20695 -0.074273 0.75808 -0.34243\n",
      "a -0.27086 0.044006 -0.02026 -0.17395 0.6444 0.71213 0.3551 0.47138 -0.29637 0.54427 -0.72294 -0.0047612 0.040611 0.043236 0.29729 0.10725 0.40156 -0.53662 0.033382 0.067396 0.64556 -0.085523 0.14103 0.094539 0.74947 -0.194 -0.68739 -0.41741 -0.22807 0.12 -0.48999 0.80945 0.045138 -0.11898 0.20161 0.39276 -0.20121 0.31354 0.75304 0.25907 -0.11566 -0.029319 0.93499 -0.36067 0.5242 0.23706 0.52715 0.22869 -0.51958 -0.79349 -0.20368 -0.50187 0.18748 0.94282 -0.44834 -3.6792 0.044183 -0.26751 2.1997 0.241 -0.033425 0.69553 -0.64472 -0.0072277 0.89575 0.20015 0.46493 0.61933 -0.1066 0.08691 -0.4623 0.18262 -0.15849 0.020791 0.19373 0.063426 -0.31673 -0.48177 -1.3848 0.13669 0.96859 0.049965 -0.2738 -0.035686 -1.0577 -0.24467 0.90366 -0.12442 0.080776 -0.83401 0.57201 0.088945 -0.42532 -0.018253 -0.079995 -0.28581 -0.01089 -0.4923 0.63687 0.23642\n",
      "\" -0.30457 -0.23645 0.17576 -0.72854 -0.28343 -0.2564 0.26587 0.025309 -0.074775 -0.3766 -0.057774 0.12159 0.34384 0.41928 -0.23236 -0.31547 0.60939 0.25117 -0.68667 0.70873 1.2162 -0.1824 -0.48442 -0.33445 0.30343 1.086 0.49992 -0.20198 0.27959 0.68352 -0.33566 -0.12405 0.059656 0.33617 0.37501 0.56552 0.44867 0.11284 -0.16196 -0.94346 -0.67961 0.18581 0.060653 0.43776 0.13834 -0.48207 -0.56141 -0.25422 -0.52445 0.097003 -0.48925 0.19077 0.21481 1.4969 -0.86665 -3.2846 0.56854 0.41971 1.2294 0.78522 -0.29369 0.63803 -1.5926 -0.20437 1.5306 0.13548 0.50722 0.18742 0.48552 -0.28995 0.19573 0.0046515 0.092879 -0.42444 0.64987 0.52839 0.077908 0.8263 -1.2208 -0.34955 0.49855 -0.64155 -0.72308 0.26566 -1.3643 -0.46364 -0.52048 -1.0525 0.22895 -0.3456 -0.658 -0.16735 0.35158 0.74337 0.26074 0.061104 -0.39079 -0.84557 -0.035432 0.17036\n",
      "'s 0.58854 -0.2025 0.73479 -0.68338 -0.19675 -0.1802 -0.39177 0.34172 -0.60561 0.63816 -0.26695 0.36486 -0.40379 -0.1134 -0.58718 0.2838 0.8025 -0.35303 0.30083 0.078935 0.44416 -0.45906 0.79294 0.50365 0.32805 0.28027 -0.4933 -0.38482 -0.039284 -0.2483 -0.1988 1.1469 0.13228 0.91691 -0.36739 0.89425 0.5426 0.61738 -0.62205 -0.31132 -0.50933 0.23335 1.0826 -0.044637 -0.12767 0.27628 -0.032617 -0.27397 0.77764 -0.50861 0.038307 -0.33679 0.42344 1.2271 -0.53826 -3.2411 0.42626 0.025189 1.3948 0.65085 0.03325 0.37141 0.4044 0.35558 0.98265 -0.61724 0.53901 0.76219 0.30689 0.33065 0.30956 -0.15161 -0.11313 -0.81281 0.6145 -0.44341 -0.19163 -0.089551 -1.5927 0.37405 0.85857 0.54613 -0.31928 0.52598 -1.4802 -0.97931 -0.2939 -0.14724 0.25803 -0.1817 1.0149 0.77649 0.12598 0.54779 -1.0316 0.064599 -0.37523 -0.94475 0.61802 0.39591\n",
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x  #gpu\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "# Download Glove (Global Vectors for Word Representation)\n",
    "!wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "!ls -l\n",
    "!head -n 10 glove.6B.100d.txt\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2020,
     "status": "ok",
     "timestamp": 1592122253064,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "-FHNl-LEp_-L",
    "outputId": "d17d4e54-2c8e-46a2-a78c-6b2a2866f39d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "data = pd.read_csv('/gdrive/My Drive/sourcecode/vtc_ai/raw/12 imdb_dataset.csv')\n",
    "\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7agH2iqxk9l",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Clean data\n",
    "\n",
    "Raw data:\n",
    "\n",
    "```\n",
    "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\n",
    "```\n",
    "\n",
    "The raw data contains invaluable characters/tokens such as dot, question mark,  HTML tag, etc... We should clean this first\n",
    "\n",
    "Cleaned data should look like this\n",
    "\n",
    "```\n",
    "Basically there s a family where a little boy Jake thinks there s a zombie in his closet his parents are fighting all the time br br This movie is slower than a soap opera and suddenly Jake decides to become Rambo and kill the zombie br br OK first of all when you re going to make a film you must Decide if its a thriller or a drama As a drama the movie is watchable Parents are divorcing arguing like in real life And then we have Jake with his closet which totally ruins all the film I expected to see a BOOGEYMAN similar movie and instead i watched a drama with some meaningless thriller spots br br out of just for the well playing parents descent dialogs As for the shots with Jake just ignore them \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TPiT5mwDxl7U",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean(review):\n",
    "    r = review.lower()\n",
    "    # remove html tags\n",
    "    r = re.sub('<.*?>', '', r)\n",
    "    # replace non-word characters with spaces\n",
    "    r = re.sub('(\\W|\\d)+', ' ', r)\n",
    "    return r\n",
    "\n",
    "data['review_cleaned'] = data.review.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6433,
     "status": "ok",
     "timestamp": 1592122265496,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "ER3WjKqt5YiF",
    "outputId": "77da84df-fc3c-476c-98e2-2c442c158f04",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there s a family where a little boy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter mattei s love in the time of money is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>probably my all time favorite movie a story of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i sure would like to see a resurrection of a u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>this show was an amazing fresh innovative idea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "      <td>encouraged by the positive comments about this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>if you like original gut wrenching laughter yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  ...                                     review_cleaned\n",
       "0  One of the other reviewers has mentioned that ...  ...  one of the other reviewers has mentioned that ...\n",
       "1  A wonderful little production. <br /><br />The...  ...  a wonderful little production the filming tech...\n",
       "2  I thought this was a wonderful way to spend ti...  ...  i thought this was a wonderful way to spend ti...\n",
       "3  Basically there's a family where a little boy ...  ...  basically there s a family where a little boy ...\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  ...  petter mattei s love in the time of money is a...\n",
       "5  Probably my all-time favorite movie, a story o...  ...  probably my all time favorite movie a story of...\n",
       "6  I sure would like to see a resurrection of a u...  ...  i sure would like to see a resurrection of a u...\n",
       "7  This show was an amazing, fresh & innovative i...  ...  this show was an amazing fresh innovative idea...\n",
       "8  Encouraged by the positive comments about this...  ...  encouraged by the positive comments about this...\n",
       "9  If you like original gut wrenching laughter yo...  ...  if you like original gut wrenching laughter yo...\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "supsJC1o-gqs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "# sentence_start_token = \"SENTENCE_START\"\n",
    "# sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1290,
     "status": "ok",
     "timestamp": 1592122285645,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "CkEZP0Yt9W4Y",
    "outputId": "b445b7e9-f54d-4a71-c7af-86df1564b6f4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'other',\n",
       " 'reviewers',\n",
       " 'has',\n",
       " 'mentioned',\n",
       " 'that',\n",
       " 'after',\n",
       " 'watching',\n",
       " 'just',\n",
       " 'oz',\n",
       " 'episode',\n",
       " 'you',\n",
       " 'll',\n",
       " 'be',\n",
       " 'hooked',\n",
       " 'they',\n",
       " 'are',\n",
       " 'right',\n",
       " 'as',\n",
       " 'this',\n",
       " 'is',\n",
       " 'exactly',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'with',\n",
       " 'me',\n",
       " 'the',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'struck',\n",
       " 'me',\n",
       " 'about',\n",
       " 'oz',\n",
       " 'was',\n",
       " 'its',\n",
       " 'brutality',\n",
       " 'and',\n",
       " 'unflinching',\n",
       " 'scenes',\n",
       " 'of',\n",
       " 'violence',\n",
       " 'which',\n",
       " 'set',\n",
       " 'in',\n",
       " 'right',\n",
       " 'from',\n",
       " 'the',\n",
       " 'word',\n",
       " 'go',\n",
       " 'trust',\n",
       " 'me',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'show',\n",
       " 'for',\n",
       " 'the',\n",
       " 'faint',\n",
       " 'hearted',\n",
       " 'or',\n",
       " 'timid',\n",
       " 'this',\n",
       " 'show',\n",
       " 'pulls',\n",
       " 'no',\n",
       " 'punches',\n",
       " 'with',\n",
       " 'regards',\n",
       " 'to',\n",
       " 'drugs',\n",
       " 'sex',\n",
       " 'or',\n",
       " 'violence',\n",
       " 'its',\n",
       " 'is',\n",
       " 'hardcore',\n",
       " 'in',\n",
       " 'the',\n",
       " 'classic',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'word',\n",
       " 'it',\n",
       " 'is',\n",
       " 'called',\n",
       " 'oz',\n",
       " 'as',\n",
       " 'that',\n",
       " 'is',\n",
       " 'the',\n",
       " 'nickname',\n",
       " 'given',\n",
       " 'to',\n",
       " 'the',\n",
       " 'oswald',\n",
       " 'maximum',\n",
       " 'security',\n",
       " 'state',\n",
       " 'penitentary',\n",
       " 'it',\n",
       " 'focuses',\n",
       " 'mainly',\n",
       " 'on',\n",
       " 'emerald',\n",
       " 'city',\n",
       " 'an',\n",
       " 'experimental',\n",
       " 'section',\n",
       " 'of',\n",
       " 'the',\n",
       " 'prison',\n",
       " 'where',\n",
       " 'all',\n",
       " 'the',\n",
       " 'cells',\n",
       " 'have',\n",
       " 'glass',\n",
       " 'fronts',\n",
       " 'and',\n",
       " 'face',\n",
       " 'inwards',\n",
       " 'so',\n",
       " 'privacy',\n",
       " 'is',\n",
       " 'not',\n",
       " 'high',\n",
       " 'on',\n",
       " 'the',\n",
       " 'agenda',\n",
       " 'em',\n",
       " 'city',\n",
       " 'is',\n",
       " 'home',\n",
       " 'to',\n",
       " 'many',\n",
       " 'aryans',\n",
       " 'muslims',\n",
       " 'gangstas',\n",
       " 'latinos',\n",
       " 'christians',\n",
       " 'italians',\n",
       " 'irish',\n",
       " 'and',\n",
       " 'more',\n",
       " 'so',\n",
       " 'scuffles',\n",
       " 'death',\n",
       " 'stares',\n",
       " 'dodgy',\n",
       " 'dealings',\n",
       " 'and',\n",
       " 'shady',\n",
       " 'agreements',\n",
       " 'are',\n",
       " 'never',\n",
       " 'far',\n",
       " 'away',\n",
       " 'i',\n",
       " 'would',\n",
       " 'say',\n",
       " 'the',\n",
       " 'main',\n",
       " 'appeal',\n",
       " 'of',\n",
       " 'the',\n",
       " 'show',\n",
       " 'is',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'it',\n",
       " 'goes',\n",
       " 'where',\n",
       " 'other',\n",
       " 'shows',\n",
       " 'wouldn',\n",
       " 't',\n",
       " 'dare',\n",
       " 'forget',\n",
       " 'pretty',\n",
       " 'pictures',\n",
       " 'painted',\n",
       " 'for',\n",
       " 'mainstream',\n",
       " 'audiences',\n",
       " 'forget',\n",
       " 'charm',\n",
       " 'forget',\n",
       " 'romance',\n",
       " 'oz',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'mess',\n",
       " 'around',\n",
       " 'the',\n",
       " 'first',\n",
       " 'episode',\n",
       " 'i',\n",
       " 'ever',\n",
       " 'saw',\n",
       " 'struck',\n",
       " 'me',\n",
       " 'as',\n",
       " 'so',\n",
       " 'nasty',\n",
       " 'it',\n",
       " 'was',\n",
       " 'surreal',\n",
       " 'i',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'say',\n",
       " 'i',\n",
       " 'was',\n",
       " 'ready',\n",
       " 'for',\n",
       " 'it',\n",
       " 'but',\n",
       " 'as',\n",
       " 'i',\n",
       " 'watched',\n",
       " 'more',\n",
       " 'i',\n",
       " 'developed',\n",
       " 'a',\n",
       " 'taste',\n",
       " 'for',\n",
       " 'oz',\n",
       " 'and',\n",
       " 'got',\n",
       " 'accustomed',\n",
       " 'to',\n",
       " 'the',\n",
       " 'high',\n",
       " 'levels',\n",
       " 'of',\n",
       " 'graphic',\n",
       " 'violence',\n",
       " 'not',\n",
       " 'just',\n",
       " 'violence',\n",
       " 'but',\n",
       " 'injustice',\n",
       " 'crooked',\n",
       " 'guards',\n",
       " 'who',\n",
       " 'll',\n",
       " 'be',\n",
       " 'sold',\n",
       " 'out',\n",
       " 'for',\n",
       " 'a',\n",
       " 'nickel',\n",
       " 'inmates',\n",
       " 'who',\n",
       " 'll',\n",
       " 'kill',\n",
       " 'on',\n",
       " 'order',\n",
       " 'and',\n",
       " 'get',\n",
       " 'away',\n",
       " 'with',\n",
       " 'it',\n",
       " 'well',\n",
       " 'mannered',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'inmates',\n",
       " 'being',\n",
       " 'turned',\n",
       " 'into',\n",
       " 'prison',\n",
       " 'bitches',\n",
       " 'due',\n",
       " 'to',\n",
       " 'their',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'street',\n",
       " 'skills',\n",
       " 'or',\n",
       " 'prison',\n",
       " 'experience',\n",
       " 'watching',\n",
       " 'oz',\n",
       " 'you',\n",
       " 'may',\n",
       " 'become',\n",
       " 'comfortable',\n",
       " 'with',\n",
       " 'what',\n",
       " 'is',\n",
       " 'uncomfortable',\n",
       " 'viewing',\n",
       " 'thats',\n",
       " 'if',\n",
       " 'you',\n",
       " 'can',\n",
       " 'get',\n",
       " 'in',\n",
       " 'touch',\n",
       " 'with',\n",
       " 'your',\n",
       " 'darker',\n",
       " 'side']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(data.review_cleaned.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N73E9DLkztD_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenize data\n",
    "You should use the library NLTK instead of simple string splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FCIOYCw01uap",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_reviews = [nltk.word_tokenize(sent) for sent in data.review_cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZICzKpC1u7h",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Build your own dictionary\n",
    "Get a list of the n highest frequency tokens from tokenized data. Which value n should be? It depends on your experience, language knowledge, task requirements, token frequency distribution, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8825,
     "status": "ok",
     "timestamp": 1592122440685,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "b6epilpb1x_Y",
    "outputId": "2062ce49-b82d-4bcf-a463-19762426fd19",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102084 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_reviews))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1592122478373,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "l2WR43tkAaoO",
    "outputId": "3d664da3-8be4-4882-cf23-e0ee3b13a8b9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 10000.\n",
      "The least frequent word in our vocabulary is 'inheritance' and appeared 59 times.\n"
     ]
    }
   ],
   "source": [
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1004,
     "status": "ok",
     "timestamp": 1592122611911,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "zRKDrDGDDk6x",
    "outputId": "b7887bf6-7e13-4fc9-93d3-34b760ce3116",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFUvw7xd1whp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Refine tokenized data:\n",
    "\n",
    "Replace all token which does not exist in the above dictionary by special token: `UNKNOWN_TOKEN`. Remember to add this special token to our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xG2yAyC81yuC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i, sent in enumerate(tokenized_reviews):\n",
    "    tokenized_reviews[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1158,
     "status": "ok",
     "timestamp": 1592122630747,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "j6jA9LLiCrek",
    "outputId": "f485158c-d819-4baa-ab18-fee70870e89f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example sentence: 'one of the other reviewers has mentioned that after watching just oz episode you ll be hooked they are right as this is exactly what happened with me the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the word it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to many aryans muslims gangstas latinos christians italians irish and more so scuffles death stares dodgy dealings and shady agreements are never far away i would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare forget pretty pictures painted for mainstream audiences forget charm forget romance oz doesn t mess around the first episode i ever saw struck me as so nasty it was surreal i couldn t say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards who ll be sold out for a nickel inmates who ll kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side '\n",
      "\n",
      "Example sentence after Pre-processing: '['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'UNKNOWN_TOKEN', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'UNKNOWN_TOKEN', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'UNKNOWN_TOKEN', 'given', 'to', 'the', 'UNKNOWN_TOKEN', 'maximum', 'security', 'state', 'UNKNOWN_TOKEN', 'it', 'focuses', 'mainly', 'on', 'UNKNOWN_TOKEN', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'UNKNOWN_TOKEN', 'and', 'face', 'UNKNOWN_TOKEN', 'so', 'UNKNOWN_TOKEN', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'UNKNOWN_TOKEN', 'muslims', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'UNKNOWN_TOKEN', 'death', 'stares', 'dodgy', 'UNKNOWN_TOKEN', 'and', 'shady', 'UNKNOWN_TOKEN', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn', 't', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn', 't', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'couldn', 't', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'UNKNOWN_TOKEN', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'UNKNOWN_TOKEN', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'UNKNOWN_TOKEN', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side']'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample sentence: '%s'\" % data.review_cleaned.iloc[0])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qdf1D9tF1_W_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Encode tokenzied data\n",
    "\n",
    "Replace all tokens by their indices in the above dictionary. Each token will be encoded to one-hot-coding later based on its dictionary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 980,
     "status": "ok",
     "timestamp": 1592122670186,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "2xnO4qZn2rN9",
    "outputId": "2c86286c-3876-4bd5-b3fa-564eaa0a0327",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234.14006\n"
     ]
    }
   ],
   "source": [
    "total_len = sum(len(r) for r in tokenized_reviews)\n",
    "print(total_len / len(tokenized_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3258,
     "status": "ok",
     "timestamp": 1592122684201,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "ymPm3mJWEj4m",
    "outputId": "2260adac-7f9d-435b-fc91-5704dd5d86df",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'UNKNOWN_TOKEN', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'UNKNOWN_TOKEN', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'UNKNOWN_TOKEN', 'given', 'to', 'the', 'UNKNOWN_TOKEN', 'maximum', 'security', 'state', 'UNKNOWN_TOKEN', 'it', 'focuses', 'mainly', 'on', 'UNKNOWN_TOKEN', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'UNKNOWN_TOKEN', 'and', 'face', 'UNKNOWN_TOKEN', 'so', 'UNKNOWN_TOKEN', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'UNKNOWN_TOKEN', 'muslims', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'UNKNOWN_TOKEN', 'death', 'stares', 'dodgy', 'UNKNOWN_TOKEN', 'and', 'shady', 'UNKNOWN_TOKEN', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn', 't', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn', 't', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'couldn', 't', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'UNKNOWN_TOKEN', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'UNKNOWN_TOKEN', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'UNKNOWN_TOKEN', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side']\n",
      "[27, 3, 0, 76, 2037, 45, 1048, 10, 99, 148, 40, 3062, 393, 19, 228, 28, 3172, 31, 24, 202, 13, 9, 5, 610, 46, 588, 16, 67, 0, 87, 147, 10, 3215, 67, 43, 3062, 12, 90, 5320, 1, 9999, 134, 3, 558, 60, 264, 7, 202, 36, 0, 645, 140, 1719, 67, 9, 5, 22, 2, 115, 15, 0, 7803, 2310, 39, 9999, 9, 115, 2571, 55, 5844, 16, 5462, 4, 1452, 370, 39, 558, 90, 5, 3781, 7, 0, 354, 355, 3, 0, 645, 6, 5, 431, 3062, 13, 10, 5, 0, 9999, 356, 4, 0, 9999, 6785, 2543, 1028, 9999, 6, 2685, 1397, 21, 9999, 517, 33, 4637, 2435, 3, 0, 1178, 114, 29, 0, 6922, 26, 2884, 9999, 1, 384, 9999, 35, 9999, 5, 22, 296, 21, 0, 4847, 2899, 517, 5, 340, 4, 106, 9999, 8056, 9999, 9999, 4988, 7680, 2425, 1, 51, 35, 9999, 323, 8976, 7178, 9999, 1, 8576, 9999, 24, 110, 222, 239, 8, 59, 131, 0, 279, 1310, 3, 0, 115, 5, 678, 4, 0, 191, 10, 6, 265, 114, 76, 272, 567, 20, 2994, 815, 181, 1287, 4121, 15, 2472, 1210, 815, 1417, 815, 861, 3062, 151, 20, 936, 184, 0, 87, 393, 8, 122, 208, 3215, 67, 13, 35, 1602, 6, 12, 2217, 8, 410, 20, 131, 8, 12, 1565, 15, 6, 17, 13, 8, 289, 51, 8, 1401, 2, 1252, 15, 3062, 1, 183, 9999, 4, 0, 296, 2016, 3, 2113, 558, 22, 40, 558, 17, 7567, 7069, 4959, 34, 228, 28, 2955, 42, 15, 2, 9999, 6786, 34, 228, 493, 21, 625, 1, 74, 239, 16, 6, 68, 7508, 636, 691, 6786, 108, 646, 82, 1178, 9999, 678, 4, 65, 563, 3, 887, 1999, 39, 1178, 548, 148, 3062, 19, 196, 424, 3799, 16, 46, 5, 3284, 791, 1580, 44, 19, 47, 74, 7, 1195, 16, 125, 4056, 478]\n"
     ]
    }
   ],
   "source": [
    "encoded_reviews = [[word_to_index[word] for word in review] for review in tokenized_reviews]\n",
    "print(tokenized_reviews[0])\n",
    "print(encoded_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpSYjCpQFuWF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 230\n",
    "encoded_reviews = [review[: max_length] for review in encoded_reviews]\n",
    "label = [1 if s=='positive' else 0 for s in data.sentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbGmMb2F2rvh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Padding\n",
    "\n",
    "Each data point here is a review with varying sizes. So, we have to choose the max length m and:\n",
    "- Trim all reviews that contains more than m tokens\n",
    "- Add UNKNOWN_TOKEN to the end of all reviews that contains less than m tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_SVD0po4CTB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pad_word = word_to_index[unknown_token]\n",
    "for review in encoded_reviews:\n",
    "    if len(review) < max_length:\n",
    "        delta = max_length - len(review)\n",
    "        padding = [pad_word] * delta\n",
    "        review += padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2025,
     "status": "ok",
     "timestamp": 1592123373504,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "o1MX3dMA8qFb",
    "outputId": "1bde9557-c110-479d-bacd-dd7805675c58",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNKNOWN_TOKEN'"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1246,
     "status": "ok",
     "timestamp": 1592123446881,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "KhWsojgfHrGP",
    "outputId": "5500e23e-4017-4d74-c0db-06d76481b65b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_reviews[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAnBsd99IA4I",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array(encoded_reviews)\n",
    "y = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1114,
     "status": "ok",
     "timestamp": 1592123450773,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "yTi7CZBkIKUO",
    "outputId": "8059a843-f4b2-4cad-af7e-d00021958a18",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   8,  190,    9,   12,    2,  389,   94,    4, 1134,   56,   21,\n",
       "          2,   96,  853, 1450, 2572, 1220,    7,    0,  882, 9999,  754,\n",
       "          1,  148,    2,  631, 2310,  199,    0,  111,    5, 4057,   17,\n",
       "          0,  407,    5, 1888,    1,    0,  100,   24, 1466,   58,    0,\n",
       "         68, 6002, 6596, 1537,  469,  135,   48,  196,   28,  665,   50,\n",
       "         31,  942,    9,    5,   22, 1010,  216, 2894, 5099,    8,  190,\n",
       "          6,   12, 3018,   10, 2811, 1764,    5,  130, 1384,    7, 1113,\n",
       "          3,    0,  392,  106,    3,  177,   26, 2038,    4,  109,    9,\n",
       "         12,    0,   88,    8,  219, 1425,   30,   27,    3, 2811,   11,\n",
       "       1289,    7,  154, 2994,    8,  131,    2, 2039,  135,    8,  136,\n",
       "        110,   75, 1486,   16, 8117, 9999,    7,    9,   53, 1307,    4,\n",
       "       1237,  175,   41, 1229, 1404,    1, 5029,  202,   82,    2,  828,\n",
       "         17, 3515,  186,  244,    9,  196,   22,   28,    0, 7122, 5051,\n",
       "          3,   25,  612,   17,    6,   12, 9999,   71, 1796, 2909, 9999,\n",
       "          1,   51,  217,   71, 2487,    2,   79,  199,    4,  140,   64,\n",
       "         16,  347, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "       9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "       9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "       9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "       9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999,\n",
       "       9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999, 9999])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "je3SykCu4CzZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split train & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEn4F1QV4Fm2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 659,
     "status": "ok",
     "timestamp": 1592123454576,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "tJnbBWAFKmiq",
    "outputId": "70975d23-3aea-4029-b7bc-0c4ec00d5814",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 230)\n",
      "(15000, 230)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPMyarqv4GG0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 276644,
     "status": "ok",
     "timestamp": 1592123735572,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "CCcnzU5m4RCh",
    "outputId": "76de46fc-d6be-4a7b-f228-8cd8fb62f13e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.6632 - accuracy: 0.5905\n",
      "Epoch 2/15\n",
      "274/274 [==============================] - 17s 64ms/step - loss: 0.6476 - accuracy: 0.6241\n",
      "Epoch 3/15\n",
      "274/274 [==============================] - 18s 65ms/step - loss: 0.6507 - accuracy: 0.6102\n",
      "Epoch 4/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.5521 - accuracy: 0.7119\n",
      "Epoch 5/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.3561 - accuracy: 0.8548\n",
      "Epoch 6/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.2593 - accuracy: 0.9001\n",
      "Epoch 7/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.1851 - accuracy: 0.9322\n",
      "Epoch 8/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.1267 - accuracy: 0.9569\n",
      "Epoch 9/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.0849 - accuracy: 0.9740\n",
      "Epoch 10/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.0636 - accuracy: 0.9810\n",
      "Epoch 11/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.0452 - accuracy: 0.9873\n",
      "Epoch 12/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.0329 - accuracy: 0.9915\n",
      "Epoch 13/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.0338 - accuracy: 0.9898\n",
      "Epoch 14/15\n",
      "274/274 [==============================] - 17s 63ms/step - loss: 0.0335 - accuracy: 0.9905\n",
      "Epoch 15/15\n",
      "274/274 [==============================] - 17s 62ms/step - loss: 0.0598 - accuracy: 0.9810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f326b925400>"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Embedding(vocabulary_size, 300, input_length=max_length),\n",
    "                tf.keras.layers.LSTM(100),\n",
    "\n",
    "                tf.keras.layers.Flatten(),\n",
    "\n",
    "                tf.keras.layers.Dense(100, activation='relu'),\n",
    "                tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4516,
     "status": "ok",
     "timestamp": 1592123742829,
     "user": {
      "displayName": "Kevin Do",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjg4QZpiLvUAOLIxGNIufzzhtV5MRdtjDtJYd3q=s64",
      "userId": "15720740312587311195"
     },
     "user_tz": -420
    },
    "id": "zNJcvtCsP-dN",
    "outputId": "b17a5205-0180-41d4-e060-db171b731a79",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.576018214225769, 0.8396666646003723]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rcXyR-ug4RSa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training with Embedding layer and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfN6aEmrlmbp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tF2btXQDlmwz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training with Embedding layer, LSTM, Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZpesBmnRjjH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCyKB2dyQjaa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model2 = tf.keras.Sequential([\n",
    "                tf.keras.layers.Embedding(vocabulary_size, 300, input_length=max_length),\n",
    "                tf.keras.layers.LSTM(100),\n",
    "\n",
    "                tf.keras.layers.Flatten(),\n",
    "\n",
    "                tf.keras.layers.Dense(100, activation='relu'),\n",
    "                tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model2.fit(X_train, y_train, epochs=15, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IMDB_Review_v2.ipynb",
   "provenance": [
    {
     "file_id": "1I7UbXi3esBiJL7wlsCba7xY8HXHt3Gzx",
     "timestamp": 1578915603496
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}