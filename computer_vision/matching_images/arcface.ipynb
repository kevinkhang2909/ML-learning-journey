{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3a010a0-1e28-42e1-be3b-5dbe8ab3f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from func import ShopeeDataset, device, ShopeeNet, torch, get_train_transforms, np, f1_score_cal\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95538fa-3b1c-423d-aa3f-0e5580286462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>/Users/kevin/OneDrive - Seagroup/computer_viso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>/Users/kevin/OneDrive - Seagroup/computer_viso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>/Users/kevin/OneDrive - Seagroup/computer_viso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>/Users/kevin/OneDrive - Seagroup/computer_viso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>/Users/kevin/OneDrive - Seagroup/computer_viso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n",
       "\n",
       "                                            filepath  \n",
       "0  /Users/kevin/OneDrive - Seagroup/computer_viso...  \n",
       "1  /Users/kevin/OneDrive - Seagroup/computer_viso...  \n",
       "2  /Users/kevin/OneDrive - Seagroup/computer_viso...  \n",
       "3  /Users/kevin/OneDrive - Seagroup/computer_viso...  \n",
       "4  /Users/kevin/OneDrive - Seagroup/computer_viso...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path.home() / 'OneDrive - Seagroup/computer_vison/shopee_item_images/'\n",
    "path_img = path / 'train_images'\n",
    "\n",
    "df = pd.read_csv(path / 'train.csv')\n",
    "df[\"filepath\"] = df[\"image\"].map(lambda x: str(path_img / x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2cadd18-2b38-4a56-8b62-423376a1d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Len: 34,250\n",
      "Image Shape [0]: torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "dataset_data = ShopeeDataset(csv=df, train=True)\n",
    "data_loader = DataLoader(dataset_data, batch_size=16)\n",
    "\n",
    "print(f\"Dataset Len: {len(dataset_data):,}\\nImage Shape [0]: {dataset_data[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3146c446-c43c-4919-82d0-0147a0f71ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_effnet = EfficientNet.from_name(\"efficientnet-b3\").to(device)\n",
    "\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for image, label in tqdm(data_loader):\n",
    "        image = image.to(device)\n",
    "        img_embeddings = model_effnet(image)\n",
    "        img_embeddings = img_embeddings.detach().cpu().numpy()\n",
    "        embeddings.append(img_embeddings)\n",
    "        \n",
    "# Concatenate all embeddings\n",
    "all_image_embeddings = np.concatenate(embeddings)\n",
    "print(\"image_embeddings shape: {:,}/{:,}\".format(all_image_embeddings.shape[0], all_image_embeddings.shape[1]))\n",
    "\n",
    "# Clean memory\n",
    "del model_effnet\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b643150-c6d2-47ec-a5d6-64b15a0c5711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb89d25-5e42-4c4c-9d56-6d51b2eb66a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235403aa-59a2-420c-b4e2-d04330beea16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964bf79-3081-45f8-b84f-cdb19672a7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "434dfbb2-2220-4a61-9bf3-ef33fa2f95a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e26d3bfe-1159-4451-9537-6baf5ce8ef93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model building for efficientnet_b3 backbone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1071/1071 [03:55<00:00,  4.54it/s]\n"
     ]
    }
   ],
   "source": [
    "CLASSES = 11014\n",
    "BATCH_SIZE = 32\n",
    "dim = (512, 512)\n",
    "model_params = {\n",
    "    'n_classes': 11014,\n",
    "    'model_name': 'efficientnet_b3',\n",
    "    'use_fc': False,\n",
    "    'fc_dim': 512,\n",
    "    'dropout': 0.0,\n",
    "    'loss_module': 'arcface',\n",
    "    's': 30.0,\n",
    "    'margin': 0.50,\n",
    "    'ls_eps': 0.0,\n",
    "    'theta_zero': 0.785,\n",
    "    'pretrained': True\n",
    "}\n",
    "\n",
    "model_name = 'efficientnet_b3'\n",
    "model = ShopeeNet(**model_params)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "image_dataset = ShopeeDataset(image_paths=image_paths.values,transforms=get_train_transforms(dim))\n",
    "image_loader = DataLoader(image_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "embeds = []\n",
    "with torch.no_grad():\n",
    "    for img,label in tqdm(image_loader): \n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        feat, _ = model(img,label)\n",
    "        image_embeddings = feat.detach().cpu().numpy()\n",
    "        embeds.append(image_embeddings)\n",
    "image_embeddings = np.concatenate(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ac6dd2-9784-4fdc-9a23-16eb3cd76fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NearestNeighbors(n_neighbors=50)\n",
    "model.fit(image_embeddings)\n",
    "distances, indices = model.kneighbors(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e758ef2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our f1 score for threshold 0.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 1.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 2.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 3.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 4.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 5.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 6.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 7.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 8.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 9.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 10.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 11.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 12.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 13.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 14.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 15.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 16.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 17.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 18.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 19.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 20.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 21.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 22.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 23.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 24.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 25.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 26.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 27.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 28.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 29.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 30.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 31.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 32.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 33.01 is 0.045308828548734095\n",
      "Our f1 score for threshold 34.01 is 0.045308828548734095\n",
      "Our best score is 0.045308828548734095 and has a threshold 0.01\n"
     ]
    }
   ],
   "source": [
    "# Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n",
    "\n",
    "thresholds = list(np.arange(0.01, 35, 1))\n",
    "scores = []\n",
    "for threshold in thresholds:\n",
    "    predictions = []\n",
    "    for k in range(image_embeddings.shape[0]):\n",
    "        idx = np.where(distances[k] < threshold)[0]\n",
    "        ids = indices[k, idx]\n",
    "        posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n",
    "        predictions.append(posting_ids)\n",
    "    df['pred_matches'] = predictions\n",
    "    df['f1'] = f1_score_cal(df['matches'], df['pred_matches'])\n",
    "    score = df['f1'].mean()\n",
    "    print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "    scores.append(score)\n",
    "\n",
    "thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "best_threshold = max_score['thresholds'].values[0]\n",
    "best_score = max_score['scores'].values[0]\n",
    "print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "\n",
    "# Use threshold\n",
    "predictions = []\n",
    "for k in range(image_embeddings.shape[0]):\n",
    "    idx = np.where(distances[k,] < best_threshold)[0]\n",
    "    ids = indices[k,idx]\n",
    "    posting_ids = df['posting_id'].iloc[ids].values\n",
    "    predictions.append(posting_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91129c86",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['image_predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac8b9360",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_feather('test.ftr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a4a78ba-f723-4bd9-8137-934fc784d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dee38d-5bd9-4526-bf11-5f2957eda792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
