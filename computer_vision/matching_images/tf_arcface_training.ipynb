{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a010a0-1e28-42e1-be3b-5dbe8ab3f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from func import path, get_data, path_img\n",
    "from func_keras import get_model, seed_everything\n",
    "import tensorflow as tf\n",
    "import efficientnet.keras as efn\n",
    "import math\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95538fa-3b1c-423d-aa3f-0e5580286462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>filepath</th>\n",
       "      <th>target</th>\n",
       "      <th>title_edit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>C:\\Users\\PC\\OneDrive - Seagroup\\computer_vison...</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "      <td>paper bag victoria secret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>C:\\Users\\PC\\OneDrive - Seagroup\\computer_vison...</td>\n",
       "      <td>[train_3423213080, train_3386243561]</td>\n",
       "      <td>double tape 3m vhb 12 mm x 4,5 m original / do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>C:\\Users\\PC\\OneDrive - Seagroup\\computer_vison...</td>\n",
       "      <td>[train_2288590299, train_3803689425]</td>\n",
       "      <td>maling tts canned pork luncheon meat 397 gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>C:\\Users\\PC\\OneDrive - Seagroup\\computer_vison...</td>\n",
       "      <td>[train_3342059966, train_2406599165]</td>\n",
       "      <td>daster batik lengan pendek - motif acak / camp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>C:\\Users\\PC\\OneDrive - Seagroup\\computer_vison...</td>\n",
       "      <td>[train_3369186413, train_921438619]</td>\n",
       "      <td>nescafe \\xc3\\x89clair latte 220ml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n",
       "\n",
       "                                            filepath  \\\n",
       "0  C:\\Users\\PC\\OneDrive - Seagroup\\computer_vison...   \n",
       "1  C:\\Users\\PC\\OneDrive - Seagroup\\computer_vison...   \n",
       "2  C:\\Users\\PC\\OneDrive - Seagroup\\computer_vison...   \n",
       "3  C:\\Users\\PC\\OneDrive - Seagroup\\computer_vison...   \n",
       "4  C:\\Users\\PC\\OneDrive - Seagroup\\computer_vison...   \n",
       "\n",
       "                                 target  \\\n",
       "0   [train_129225211, train_2278313361]   \n",
       "1  [train_3423213080, train_3386243561]   \n",
       "2  [train_2288590299, train_3803689425]   \n",
       "3  [train_3342059966, train_2406599165]   \n",
       "4   [train_3369186413, train_921438619]   \n",
       "\n",
       "                                          title_edit  \n",
       "0                          paper bag victoria secret  \n",
       "1  double tape 3m vhb 12 mm x 4,5 m original / do...  \n",
       "2        maling tts canned pork luncheon meat 397 gr  \n",
       "3  daster batik lengan pendek - motif acak / camp...  \n",
       "4                  nescafe \\xc3\\x89clair latte 220ml  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input\n",
    "df = get_data(path / 'train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef29217-40f2-4741-860a-d8ef60f4b838",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_filenames = tf.io.gfile.glob([str(path) + '/train_record/*.*'])\n",
    "# test_filenames = tf.io.gfile.glob([str(path) + '/test_images/*.*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d08726-ef5a-4f63-943c-9653b748e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use correct image size with pretrained model\n",
    "IMAGE_SIZE = (300, 300)\n",
    "# Train-test-split size\n",
    "TRAIN_SIZE = 0.8\n",
    "# Initial learning rate\n",
    "LR = 0.001\n",
    "# ArcFace must assume a certain number of classes to optimize loss. May get better results on test set with higher N_CLASSES\n",
    "N_CLASSES = df['label_group'].nunique()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "SEED = 42\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 4\n",
    "STEPS_PER_EPOCH = len(df) * TRAIN_SIZE // BATCH_SIZE\n",
    "cosine_lr_fn = tf.keras.experimental.CosineDecay(LR, 3*STEPS_PER_EPOCH*10)\n",
    "\n",
    "# Function to seed everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e64b91-ed32-47f9-8824-7d06256ecc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArcFace parameters\n",
    "# s: norm of input feature\n",
    "# m: margin, Original paper states that m=0.5 gives best results. I found this variable to have the strongest effect when calculating final f1 score\n",
    "params = {\n",
    "    'm': 0.0001, \n",
    "    's': 15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b7cf784-9915-4dab-b601-009da9dc978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment data\n",
    "# As data was serialized to TFRecords, I directly convert TFRecords to datasets and thus cannot use Keras ImageDataGenerator\n",
    "def data_augment(posting_id, image, label_group, matches):   \n",
    "    rotate = tf.random.uniform(shape=(), minval=-0.1*np.pi, maxval=0.1*np.pi)\n",
    "    image = tfa.image.rotate(image, rotate, interpolation='bilinear', fill_mode='constant')\n",
    "    shear_x = tf.random.uniform(shape=(), minval=-0.2, maxval=0.2)\n",
    "    shear_y = tf.random.uniform(shape=(), minval=-0.2, maxval=0.2)\n",
    "    image = tfa.image.transform(image, [1.0, shear_x, 0.0, shear_y, 1.0, 0.0, 0.0, 0.0], interpolation='bilinear', fill_mode='constant')\n",
    "    translate_vec = tf.random.uniform(shape=(2,), minval=-int(0.05*IMAGE_SIZE[0]), maxval=int(0.05*IMAGE_SIZE[0]))\n",
    "    image = tfa.image.translate(image, translate_vec, interpolation='bilinear', fill_mode='constant')\n",
    "    \n",
    "    crop_size = tf.random.uniform(shape=(), minval=int(0.8*IMAGE_SIZE[0]), maxval=int(1.2*IMAGE_SIZE[0]), dtype=tf.int32)\n",
    "    image = tf.image.resize_with_crop_or_pad(image, crop_size, crop_size)  \n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    \n",
    "    image = tf.image.random_brightness(image, 0.10)\n",
    "    image = tf.image.random_hue(image, 0.01)\n",
    "    image = tf.image.random_saturation(image, 0.80, 1.20)\n",
    "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return posting_id, image, label_group, matches\n",
    "\n",
    "# Function to decode images from serialized image data from TFRecords\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "# Function to read TFRecords\n",
    "def read_tfrec(example):\n",
    "    tfrec_format = {\n",
    "        \"posting_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label_group\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"matches\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    posting_id = example['posting_id']\n",
    "    image = decode_image(example['image'])\n",
    "    label_group = tf.cast(example['label_group'], tf.int32)\n",
    "    matches = example['matches']\n",
    "    return posting_id, image, label_group, matches\n",
    "\n",
    "# Function to create a dataset by reading TFRecords\n",
    "def load_dataset(filenames):\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
    "    dataset = dataset.map(read_tfrec, num_parallel_calls = AUTO) \n",
    "    return dataset\n",
    "\n",
    "# Function to reformat dataset for model\n",
    "def arcface_format(posting_id, image, label_group, matches):\n",
    "    return posting_id, {'image': image, 'label': label_group}, label_group, matches\n",
    "\n",
    "# Function to construct dataset\n",
    "def get_dataset(filenames, training=False):\n",
    "    dataset = load_dataset(filenames)\n",
    "    if training:\n",
    "        ignore_order = tf.data.Options()\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042ffe94-9674-4db4-aaca-30425f2ffe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "train, valid = train_test_split(train_filenames, shuffle = True, random_state = SEED)\n",
    "train_dataset = get_dataset(train, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99bc4c0-1dbb-4808-acad-2ea1997085c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image (InputLayer)             [(None, 300, 300, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " efficientnetb3 (Functional)    (None, None, None,   10783535    ['image[0][0]']                  \n",
      "                                1536)                                                             \n",
      "                                                                                                  \n",
      " ge_m_pooling_layer (GeMPooling  (None, 1536)        0           ['efficientnetb3[0][0]']         \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          786944      ['ge_m_pooling_layer[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 512)         2048        ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " label (InputLayer)             [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " arc_margin_product (ArcMarginP  (None, 11014)       5639168     ['batch_normalization[0][0]',    \n",
      " roduct)                                                          'label[0][0]']                  \n",
      "                                                                                                  \n",
      " softmax (Softmax)              (None, 11014)        0           ['arc_margin_product[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,211,695\n",
      "Trainable params: 17,123,368\n",
      "Non-trainable params: 88,327\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(params, N_CLASSES, IMAGE_SIZE, cosine_lr_fn)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c413300-980a-4373-8702-3b9c4314611d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "6850/6850 [==============================] - 823s 117ms/step - loss: 10.4247 - sparse_categorical_accuracy: 2.1898e-04 - val_loss: 10.3030 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "6850/6850 [==============================] - 800s 117ms/step - loss: 9.8540 - sparse_categorical_accuracy: 3.6496e-04 - val_loss: 10.3141 - val_sparse_categorical_accuracy: 6.5703e-04\n",
      "Epoch 3/30\n",
      "6850/6850 [==============================] - 798s 117ms/step - loss: 9.6082 - sparse_categorical_accuracy: 7.6642e-04 - val_loss: 10.1989 - val_sparse_categorical_accuracy: 1.0951e-04\n",
      "Epoch 4/30\n",
      "6850/6850 [==============================] - 803s 117ms/step - loss: 9.4562 - sparse_categorical_accuracy: 0.0012 - val_loss: 9.7375 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "6850/6850 [==============================] - 803s 117ms/step - loss: 9.3059 - sparse_categorical_accuracy: 0.0023 - val_loss: 9.5313 - val_sparse_categorical_accuracy: 3.2852e-04\n",
      "Epoch 6/30\n",
      "6850/6850 [==============================] - 802s 117ms/step - loss: 9.1715 - sparse_categorical_accuracy: 0.0031 - val_loss: 9.7764 - val_sparse_categorical_accuracy: 3.2852e-04\n",
      "Epoch 7/30\n",
      "6850/6850 [==============================] - 804s 117ms/step - loss: 9.0544 - sparse_categorical_accuracy: 0.0030 - val_loss: 9.6282 - val_sparse_categorical_accuracy: 0.0028\n",
      "Epoch 8/30\n",
      "6850/6850 [==============================] - 807s 118ms/step - loss: 8.9721 - sparse_categorical_accuracy: 0.0046 - val_loss: 9.5844 - val_sparse_categorical_accuracy: 2.1901e-04\n",
      "Epoch 9/30\n",
      "6850/6850 [==============================] - 813s 119ms/step - loss: 8.9006 - sparse_categorical_accuracy: 0.0047 - val_loss: 9.5798 - val_sparse_categorical_accuracy: 0.0013\n",
      "Epoch 10/30\n",
      "6850/6850 [==============================] - 806s 118ms/step - loss: 8.8237 - sparse_categorical_accuracy: 0.0042 - val_loss: 9.8400 - val_sparse_categorical_accuracy: 1.0951e-04\n",
      "Epoch 11/30\n",
      "6850/6850 [==============================] - 807s 118ms/step - loss: 8.7642 - sparse_categorical_accuracy: 0.0056 - val_loss: 9.5805 - val_sparse_categorical_accuracy: 4.3802e-04\n",
      "Epoch 12/30\n",
      "6850/6850 [==============================] - 806s 118ms/step - loss: 8.7015 - sparse_categorical_accuracy: 0.0055 - val_loss: 9.5299 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "2392/6850 [=========>....................] - ETA: 8:17 - loss: 8.6494 - sparse_categorical_accuracy: 0.0047"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "seed_everything(seed)\n",
    "train, valid = train_test_split(train_filenames, shuffle = True, random_state = seed)\n",
    "train_dataset = get_dataset(train, training=True)\n",
    "train_dataset = train_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
    "val_dataset = get_dataset(valid)\n",
    "val_dataset = val_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = get_model(params, N_CLASSES, IMAGE_SIZE, cosine_lr_fn)\n",
    "# Model checkpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(f\"EfficientNetB3_{IMAGE_SIZE[0]}_{SEED}_m{params['m']}_s{params['s']}.h5\",\n",
    "                                                monitor = 'val_loss',\n",
    "                                                save_best_only = True,\n",
    "                                                save_weights_only = True,\n",
    "                                                mode = 'min')\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    epochs = EPOCHS,\n",
    "                    callbacks = [checkpoint],\n",
    "                    validation_data = val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae2258-899b-4047-aaed-1a177ff8d69c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}