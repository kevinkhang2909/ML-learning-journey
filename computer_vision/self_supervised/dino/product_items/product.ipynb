{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-19T09:38:12.689403Z",
     "end_time": "2023-04-19T09:38:18.826871Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from computer_vision.self_supervised.dino.func import ImageData, CollateFn, CollateSingleImage, ImageOriginalData, Model, clip_loss, LightningModel, Config\n",
    "from lightning import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-19T09:38:18.829861Z",
     "end_time": "2023-04-19T09:38:19.135838Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path.home() / 'OneDrive - Seagroup/ai/computer_vison/shopee_price_matching/shopee_ds/images'\n",
    "files = [str(file) for file in path.glob(\"*.jpg\")]\n",
    "\n",
    "train_files, valid_files = train_test_split(files, test_size=0.15, random_state=42)\n",
    "\n",
    "train_data = ImageData(train_files)\n",
    "train_dl = DataLoader(\n",
    "    train_data,\n",
    "    Config.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=Config.num_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=CollateFn(),\n",
    ")\n",
    "\n",
    "valid_data = ImageOriginalData(valid_files)\n",
    "valid_dl = DataLoader(\n",
    "    valid_data,\n",
    "    Config.batch_size*2,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=Config.num_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=CollateSingleImage(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[1.3927, 1.3927, 1.3927,  ..., 1.6667, 1.6667, 1.6838],\n          [1.3927, 1.3927, 1.3927,  ..., 1.6838, 1.6838, 1.7009],\n          [1.3927, 1.3927, 1.3927,  ..., 1.6667, 1.6667, 1.6667],\n          ...,\n          [1.0844, 1.0844, 1.0844,  ..., 1.3755, 1.1015, 0.7419],\n          [1.0844, 1.0844, 1.0673,  ..., 0.9646, 0.9988, 1.0502],\n          [1.0673, 1.0673, 1.0502,  ..., 0.8276, 1.2899, 0.6734]],\n \n         [[1.5357, 1.5357, 1.5357,  ..., 1.8158, 1.8158, 1.8333],\n          [1.5357, 1.5357, 1.5357,  ..., 1.8333, 1.8333, 1.8508],\n          [1.5357, 1.5357, 1.5357,  ..., 1.8158, 1.8158, 1.8158],\n          ...,\n          [1.2206, 1.2206, 1.2206,  ..., 1.5007, 1.2206, 0.8529],\n          [1.2206, 1.2206, 1.2031,  ..., 1.0980, 1.1155, 1.1681],\n          [1.2206, 1.2206, 1.1856,  ..., 0.9580, 1.3957, 0.7304]],\n \n         [[1.8383, 1.8383, 1.8383,  ..., 2.1171, 2.1171, 2.1520],\n          [1.8383, 1.8383, 1.8383,  ..., 2.1520, 2.1346, 2.1520],\n          [1.8383, 1.8383, 1.8383,  ..., 2.1346, 2.1171, 2.1171],\n          ...,\n          [1.5768, 1.5768, 1.5768,  ..., 1.8034, 1.5071, 1.1237],\n          [1.5768, 1.5768, 1.5594,  ..., 1.4374, 1.4200, 1.4548],\n          [1.5768, 1.5768, 1.5420,  ..., 1.2980, 1.6988, 1.0191]]]),\n tensor([[[1.4098, 1.4098, 1.4098,  ..., 1.4954, 1.5297, 1.5639],\n          [1.4269, 1.4098, 1.4098,  ..., 1.5297, 1.5297, 1.5297],\n          [1.4269, 1.4098, 1.4098,  ..., 1.5125, 1.5297, 1.5468],\n          ...,\n          [1.5982, 1.6324, 1.6324,  ..., 1.8379, 1.8379, 1.8379],\n          [1.5810, 1.6153, 1.6324,  ..., 1.8379, 1.8379, 1.8379],\n          [1.5639, 1.5982, 1.6324,  ..., 1.8379, 1.8379, 1.8379]],\n \n         [[1.5532, 1.5532, 1.5532,  ..., 1.6408, 1.6758, 1.7108],\n          [1.5707, 1.5532, 1.5532,  ..., 1.6758, 1.6758, 1.6758],\n          [1.5707, 1.5532, 1.5532,  ..., 1.6583, 1.6758, 1.6933],\n          ...,\n          [1.7633, 1.7633, 1.7633,  ..., 1.9734, 1.9734, 1.9734],\n          [1.7458, 1.7458, 1.7633,  ..., 1.9734, 1.9734, 1.9734],\n          [1.7283, 1.7283, 1.7633,  ..., 1.9734, 1.9734, 1.9734]],\n \n         [[1.8557, 1.8557, 1.8557,  ..., 1.9428, 1.9777, 2.0125],\n          [1.8731, 1.8557, 1.8557,  ..., 1.9777, 1.9777, 1.9777],\n          [1.8731, 1.8557, 1.8557,  ..., 1.9603, 1.9777, 1.9951],\n          ...,\n          [2.1868, 2.2043, 2.2043,  ..., 2.2740, 2.2740, 2.2740],\n          [2.1694, 2.1868, 2.2043,  ..., 2.2740, 2.2740, 2.2740],\n          [2.1520, 2.1694, 2.2043,  ..., 2.2740, 2.2740, 2.2740]]]))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T09:38:19.137831Z",
     "end_time": "2023-04-19T09:38:19.212581Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1.4098, 1.3755, 1.4440,  ..., 1.0502, 0.9646, 1.1700],\n         [1.2728, 1.3927, 1.4098,  ..., 1.1872, 0.9817, 1.1015],\n         [1.3584, 1.3755, 1.4098,  ..., 1.2214, 0.9988, 1.0673],\n         ...,\n         [1.3755, 1.4612, 1.3070,  ..., 1.0159, 1.2899, 1.2385],\n         [1.4440, 1.3755, 1.2899,  ..., 1.2214, 1.2728, 1.2385],\n         [1.4612, 1.3584, 1.3413,  ..., 1.1700, 1.3242, 1.2214]],\n\n        [[1.5707, 1.5357, 1.6057,  ..., 1.1856, 1.1155, 1.3081],\n         [1.4307, 1.5532, 1.5707,  ..., 1.3256, 1.1155, 1.2381],\n         [1.5182, 1.5357, 1.5707,  ..., 1.3606, 1.1331, 1.2031],\n         ...,\n         [1.5182, 1.6057, 1.4482,  ..., 1.1856, 1.4657, 1.4132],\n         [1.5882, 1.5182, 1.4307,  ..., 1.3957, 1.4482, 1.4132],\n         [1.6057, 1.5007, 1.4832,  ..., 1.3431, 1.5007, 1.3957]],\n\n        [[1.8208, 1.7860, 1.8557,  ..., 1.4897, 1.4025, 1.6117],\n         [1.6814, 1.7860, 1.8208,  ..., 1.6291, 1.4200, 1.5420],\n         [1.7511, 1.7685, 1.8034,  ..., 1.6640, 1.4374, 1.5071],\n         ...,\n         [1.8208, 1.9080, 1.7511,  ..., 1.4897, 1.7685, 1.7163],\n         [1.8905, 1.8208, 1.7337,  ..., 1.6988, 1.7511, 1.7163],\n         [1.9080, 1.8034, 1.7860,  ..., 1.6465, 1.8034, 1.6988]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T09:38:19.214574Z",
     "end_time": "2023-04-19T09:38:19.251451Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-04-19T09:38:19.245472Z",
     "end_time": "2023-04-19T09:41:41.017213Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Kevin\\miniconda3\\envs\\hehe\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | Model | 34.5 M\n",
      "--------------------------------\n",
      "34.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "34.5 M    Total params\n",
      "68.926    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf20b3c82bec4d1ab07f999f5a2ecf09"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "425dc9c1ae0f48a39fd337a699db059d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\miniconda3\\envs\\hehe\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "teacher = Model()\n",
    "epochs = 2\n",
    "lightning_model = LightningModel(\n",
    "    model=teacher,\n",
    "    learning_rate=1e-6,\n",
    "    loss_fn=clip_loss,\n",
    "    valid_files=valid_files,\n",
    "    max_epochs=epochs,\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=epochs,\n",
    "    precision=16,\n",
    "    deterministic=True,\n",
    "    callbacks=[lr_monitor],\n",
    ")\n",
    "trainer.fit(lightning_model, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/235 [00:00<?, ?it/s]\u001B[A"
     ]
    }
   ],
   "source": [
    "image_orig_data = ImageOriginalData(valid_files)\n",
    "image_orig_dl = DataLoader(\n",
    "    image_orig_data,\n",
    "    Config.batch_size*2,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=Config.num_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=CollateSingleImage(),\n",
    ")\n",
    "\n",
    "device = 'cuda'\n",
    "teacher = teacher.eval().to(device)\n",
    "embedding = []\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(image_orig_dl):\n",
    "        out = teacher(x.to(device))\n",
    "        embedding.append(out.cpu())\n",
    "    embedding = torch.cat(embedding, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "resize = A.Resize(256, 256)\n",
    "\n",
    "\n",
    "def get_closest(embedding: torch.FloatTensor, i: int):\n",
    "    similarity = embedding @ embedding[i,:].T\n",
    "    scores, idx = similarity.topk(5)\n",
    "    return scores, idx\n",
    "\n",
    "def read_image(file):\n",
    "    image = cv2.imread(file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def plot_closest_pairs(embedding, i, files):\n",
    "    img = resize(image=read_image(files[i]))\n",
    "    plt.imshow(img['image'])\n",
    "    scores, idx = get_closest(embedding, i)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(idx), figsize=(12, 5))\n",
    "    for i, score, ax in zip(idx, scores, axes):\n",
    "        img = resize(image=read_image(files[i]))\n",
    "        ax.imshow(img['image'])\n",
    "        ax.set_title(f\"Score: {score:.2f}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "i = 1\n",
    "plot_closest_pairs(embedding, i, valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
