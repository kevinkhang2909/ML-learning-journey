{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForImageClassification, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from transformers import AutoFeatureExtractor, pipeline\n",
    "from pathlib import Path\n",
    "from evaluate import evaluator\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from time import perf_counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_id = str(Path.home() / 'PycharmProjects/ML-learning-journey/computer_vision/vit_transformers/dog_cat/vit_dog_cat')\n",
    "onnx_path = Path(\"onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:172: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "C:\\Users\\Kevin\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:177: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height != self.image_size[0] or width != self.image_size[1]:\n"
     ]
    }
   ],
   "source": [
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForImageClassification.from_pretrained(model_id, from_transformers=True)\n",
    "preprocessor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "preprocessor.save_pretrained(onnx_path)\n",
    "\n",
    "vanilla_clf = pipeline(\"image-classification\", model=model, feature_extractor=preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create ORTQuantizer and define quantization configuration\n",
    "dynamic_quantizer = ORTQuantizer.from_pretrained(model)\n",
    "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "model_quantized_path = dynamic_quantizer.quantize(save_dir=onnx_path,quantization_config=dqconfig)\n",
    "\n",
    "# load model\n",
    "model = ORTModelForImageClassification.from_pretrained(onnx_path, file_name=\"model_quantized.onnx\")\n",
    "preprocessor = AutoFeatureExtractor.from_pretrained(onnx_path)\n",
    "q8_clf = pipeline(\"image-classification\", model=model, feature_extractor=preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Testing model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.onnx file size: 327.55 MB\n",
      "model_quantized.onnx file size: 84.76 MB\n"
     ]
    }
   ],
   "source": [
    "for i in ['model.onnx', 'model_quantized.onnx']:\n",
    "    size = os.path.getsize(onnx_path / i)/(1024*1024)\n",
    "    print(f\"{i} file size: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c70ce318d9493699186071b219e5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-d5564b158da7eecc\n",
      "WARNING:datasets.builder:Found cached dataset imagefolder (C:/Users/Kevin/.cache/huggingface/datasets/imagefolder/default-d5564b158da7eecc/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model: 99.68%\n"
     ]
    }
   ],
   "source": [
    "path_train = Path.home() / 'Desktop/dogs-vs-cats/train'\n",
    "dataset_train = load_dataset(\"imagefolder\", data_dir=str(path_train), split='train')\n",
    "splits = dataset_train.train_test_split(test_size=0.2)\n",
    "dataset_test_valid = splits['test'].train_test_split(test_size=0.5)\n",
    "test_data = dataset_test_valid['test']\n",
    "\n",
    "results = evaluator(\"image-classification\").compute(\n",
    "    model_or_pipeline=q8_clf,\n",
    "    data=test_data,\n",
    "    metric=\"accuracy\",\n",
    "    input_column=\"image\",\n",
    "    label_column=\"label\",\n",
    "    label_mapping=model.config.label2id,\n",
    "    strategy=\"simple\",\n",
    ")\n",
    "\n",
    "print(f\"Quantized model: {results['accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model: P95 latency (ms) - 129.97103999978208; Average latency (ms) - 65.48 +\\- 26.57;\n",
      "Quantized model: P95 latency (ms) - 42.099455000243275; Average latency (ms) - 32.98 +\\- 7.00;\n",
      "Improvement through quantization: 3.09x\n"
     ]
    }
   ],
   "source": [
    "def measure_latency(pipe):\n",
    "    # prepare date\n",
    "    inputs = pipe.feature_extractor(images=test_data[0]['image'], return_tensors=\"pt\")\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe.model(**inputs)\n",
    "    # timed run\n",
    "    for _ in range(200):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe.model(**inputs)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies, 95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms\n",
    "\n",
    "\n",
    "vanilla_model = measure_latency(vanilla_clf)\n",
    "quantized_model = measure_latency(q8_clf)\n",
    "\n",
    "print(f\"Vanilla model: {vanilla_model[0]}\")\n",
    "print(f\"Quantized model: {quantized_model[0]}\")\n",
    "print(f\"Improvement through quantization: {round(vanilla_model[1]/quantized_model[1],2)}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
