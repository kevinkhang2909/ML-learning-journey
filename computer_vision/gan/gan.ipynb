{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-09T10:21:38.852482Z",
     "start_time": "2023-05-09T10:21:37.182454600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds = load_dataset('beans', split='train')\n",
    "ex = ds[400]\n",
    "ex"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-09T10:21:38.853481700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "data_transforms = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(), # Scales data into [0,1]\n",
    "    # T.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1]\n",
    "])\n",
    "\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [data_transforms(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return examples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T03:55:04.435139300Z",
     "start_time": "2023-05-09T03:55:04.405140300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "ds.set_transform(transforms)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T03:55:08.906440600Z",
     "start_time": "2023-05-09T03:55:08.884441400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "{'image_file_path': 'C:\\\\Users\\\\Kevin\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\0c08c0df28e72cf903074cf8a5423199131cd3c3bf5c7552d4a70a3bd8e8028a\\\\train\\\\angular_leaf_spot\\\\angular_leaf_spot_train.0.jpg',\n 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500>,\n 'labels': 0,\n 'pixel_values': tensor([[[0.2157, 0.5333, 0.7922,  ..., 0.5490, 0.5490, 0.3961],\n          [0.1843, 0.4941, 0.7294,  ..., 0.3216, 0.4000, 0.3451],\n          [0.1569, 0.4235, 0.6118,  ..., 0.4392, 0.5373, 0.4235],\n          ...,\n          [0.1137, 0.1294, 0.1176,  ..., 0.6039, 0.3922, 0.5569],\n          [0.1569, 0.1490, 0.1451,  ..., 0.4745, 0.4118, 0.5373],\n          [0.2039, 0.1725, 0.1569,  ..., 0.5686, 0.5373, 0.5333]],\n \n         [[0.1373, 0.3961, 0.6588,  ..., 0.4118, 0.4431, 0.2392],\n          [0.1176, 0.3569, 0.5765,  ..., 0.2078, 0.2941, 0.1961],\n          [0.0980, 0.2824, 0.4314,  ..., 0.2706, 0.3569, 0.2549],\n          ...,\n          [0.0392, 0.0431, 0.0431,  ..., 0.4275, 0.2706, 0.5137],\n          [0.0627, 0.0588, 0.0667,  ..., 0.3608, 0.3098, 0.4824],\n          [0.0902, 0.0706, 0.0667,  ..., 0.6157, 0.5059, 0.5216]],\n \n         [[0.0196, 0.3608, 0.6627,  ..., 0.2510, 0.2588, 0.1686],\n          [0.0157, 0.3137, 0.5451,  ..., 0.1098, 0.1608, 0.1490],\n          [0.0118, 0.2000, 0.3373,  ..., 0.2118, 0.2745, 0.2039],\n          ...,\n          [0.0118, 0.0118, 0.0157,  ..., 0.3765, 0.2039, 0.2588],\n          [0.0235, 0.0157, 0.0275,  ..., 0.2627, 0.1922, 0.2784],\n          [0.0510, 0.0235, 0.0235,  ..., 0.3137, 0.2706, 0.3020]]])}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T03:55:09.143440200Z",
     "start_time": "2023-05-09T03:55:09.111440400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "channels = 3 # suggested default : 1, number of image channels (gray scale)\n",
    "img_size = 64 # suggested default : 28, size of each image dimension\n",
    "img_shape = (channels, img_size, img_size) # (Channels, Image Size(H), Image Size(W))\n",
    "latent_dim = 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T03:55:57.399063700Z",
     "start_time": "2023-05-09T03:55:57.386063900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(input_features, output_features):\n",
    "            layers = [\n",
    "                nn.Linear(input_features, output_features),\n",
    "                nn.BatchNorm1d(output_features, 0.8),\n",
    "                nn.LeakyReLU(0.2, inplace=True)  # inplace=True: modify the input directly, slightly decrease the memory usage.\n",
    "            ]\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))), # np.prod(1, 28, 28) == 1*28*28\n",
    "            nn.Tanh()  # -1 to 1\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z) # (64, 100) --(model)--> (64, 784)\n",
    "        img = img.view(img.size(0), *img_shape) # img.size(0) == N(Batch Size), (N, C, H, W) == default --> (64, 1, 28, 28)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512), # (28*28, 512)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # 0 to 1\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1) #flatten -> from (64, 1, 28, 28) to (64, 1*28*28)\n",
    "        validity = self.model(img_flat) # Discriminate -> Real? or Fake? (64, 784) -> (64, 1)\n",
    "        return validity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T03:55:58.283786Z",
     "start_time": "2023-05-09T03:55:58.236785500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class GAN(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        width,\n",
    "        height,\n",
    "        latent_dim: int = 100,\n",
    "        lr: float = 0.0002,\n",
    "        b1: float = 0.5,\n",
    "        b2: float = 0.999,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # networks\n",
    "        data_shape = (channels, width, height)\n",
    "        self.generator = Generator(latent_dim=self.hparams.latent_dim, img_shape=data_shape)\n",
    "        self.discriminator = Discriminator(img_shape=data_shape)\n",
    "\n",
    "        self.validation_z = torch.randn(8, self.hparams.latent_dim)\n",
    "\n",
    "        self.example_input_array = torch.zeros(2, self.hparams.latent_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        imgs, _ = batch\n",
    "\n",
    "        optimizer_g, optimizer_d = self.optimizers()\n",
    "\n",
    "        # sample noise\n",
    "        z = torch.randn(imgs.shape[0], self.hparams.latent_dim)\n",
    "        z = z.type_as(imgs)\n",
    "\n",
    "        # train generator\n",
    "        # generate images\n",
    "        self.toggle_optimizer(optimizer_g)\n",
    "        self.generated_imgs = self(z)\n",
    "\n",
    "        # log sampled images\n",
    "        sample_imgs = self.generated_imgs[:6]\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, 0)\n",
    "\n",
    "        # ground truth result (ie: all fake)\n",
    "        # put on GPU because we created this tensor inside training_loop\n",
    "        valid = torch.ones(imgs.size(0), 1)\n",
    "        valid = valid.type_as(imgs)\n",
    "\n",
    "        # adversarial loss is binary cross-entropy\n",
    "        g_loss = self.adversarial_loss(self.discriminator(self(z)), valid)\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "        self.manual_backward(g_loss)\n",
    "        optimizer_g.step()\n",
    "        optimizer_g.zero_grad()\n",
    "        self.untoggle_optimizer(optimizer_g)\n",
    "\n",
    "        # train discriminator\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        self.toggle_optimizer(optimizer_d)\n",
    "\n",
    "        # how well can it label as real?\n",
    "        valid = torch.ones(imgs.size(0), 1)\n",
    "        valid = valid.type_as(imgs)\n",
    "\n",
    "        real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n",
    "\n",
    "        # how well can it label as fake?\n",
    "        fake = torch.zeros(imgs.size(0), 1)\n",
    "        fake = fake.type_as(imgs)\n",
    "\n",
    "        fake_loss = self.adversarial_loss(self.discriminator(self(z).detach()), fake)\n",
    "\n",
    "        # discriminator loss is the average of these\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "        self.manual_backward(d_loss)\n",
    "        optimizer_d.step()\n",
    "        optimizer_d.zero_grad()\n",
    "        self.untoggle_optimizer(optimizer_d)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        z = self.validation_z.type_as(self.generator.model[0].weight)\n",
    "\n",
    "        # log sampled images\n",
    "        sample_imgs = self(z)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T03:55:59.485487100Z",
     "start_time": "2023-05-09T03:55:59.416487300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T03:56:02.722158Z",
     "start_time": "2023-05-09T03:56:02.705157700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_epochs = 10 # suggested default = 200\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(tqdm(dataloader)):\n",
    "        # Adversarial ground truths (For more detail, refer *Read_More below)\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False) # imgs.size(0) == batch_size(1 batch) == 64, *TEST_CODE\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False) # And Variable is for caclulate gradient. In fact, you can use it, but you don't have to.\n",
    "                                                                                # requires_grad=False is default in tensor type. *Read_More\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.type(Tensor) # As mentioned, it is no longer necessary to wrap the tensor in a Variable.\n",
    "      # real_imgs = Variable(imgs.type(Tensor)) # requires_grad=False, Default! It's same.\n",
    "\n",
    "# ------------\n",
    "# Train Generator\n",
    "# ------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # sample noise 'z' as generator input\n",
    "        z = Tensor(np.random.normal(0, 1, (imgs.shape[0],latent_dim))) # Random sampling Tensor(batch_size, latent_dim) of Gaussian distribution\n",
    "        # z.shape == torch.Size([64, 100])\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        # gen_imgs.shape == torch.Size([64, 1, 28, 28])\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid) # torch.nn.BCELoss() compare result(64x1) and valid(64x1, filled with 1)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "# ------------\n",
    "# Train Discriminator\n",
    "# ------------\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid) # torch.nn.BCELoss() compare result(64x1) and valid(64x1, filled with 1)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) # We are learning the discriminator now. So have to use detach()\n",
    "\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()# If didn't use detach() for gen_imgs, all weights of the generator will be calculated with backward().\n",
    "        optimizer_D.step()\n",
    "\n",
    "\n",
    "\n",
    "# ------------\n",
    "# Real Time Visualization (While Training)\n",
    "# ------------\n",
    "\n",
    "        sample_z_in_train = Tensor(np.random.normal(0, 1, (imgs.shape[0],latent_dim)))\n",
    "        # z.shape == torch.Size([64, 100])\n",
    "        sample_gen_imgs_in_train = generator(sample_z_in_train).detach().cpu()\n",
    "        # gen_imgs.shape == torch.Size([64, 1, 28, 28])\n",
    "\n",
    "        if ((i+1) % 200) == 0: # show while batch - 200/657, 400/657, 600/657\n",
    "            nrow=1\n",
    "            ncols=5\n",
    "            fig, axes = plt.subplots(nrows=nrow,ncols=ncols, figsize=(8,2))\n",
    "            plt.suptitle('EPOCH : {} | BATCH(ITERATION) : {}'.format(epoch+1, i+1))\n",
    "            for ncol in range(ncols):\n",
    "                axes[ncol].imshow(sample_gen_imgs_in_train.permute(0,2,3,1)[ncol], cmap='gray')\n",
    "                axes[ncol].axis('off')\n",
    "            plt.show()\n",
    "    print(\n",
    "        \"[Epoch: %d/%d] [Batch: %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "        % (epoch+1, n_epochs, i+1, len(dataloader), d_loss.item(), g_loss.item())\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
