{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1000,  7.4000,  0.0000],\n        [-0.2000,  5.3000,  0.0000]], dtype=torch.float64)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = np.array([\n",
    "    [0.1, 7.4, 0],\n",
    "    [-0.2, 5.3, 0],\n",
    "    [0.2, 8.2, 1],\n",
    "    [0.2, 7.7, 1]\n",
    "])\n",
    "loader = DataLoader(data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "{'x1': tensor([ 0.1000, -0.2000], dtype=torch.float64),\n 'x2': tensor([7.4000, 5.3000], dtype=torch.float64),\n 'y': tensor([0, 0])}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_data = [\n",
    "    {'x1': 0.1, 'x2': 7.4, 'y': 0},\n",
    "    {'x1': -0.2, 'x2': 5.3, 'y': 0},\n",
    "    {'x1': 0.2, 'x2': 8.2, 'y': 1},\n",
    "    {'x1': 0.2, 'x2': 7.7, 'y': 10},\n",
    "]\n",
    "loader = DataLoader(dict_data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 8\u001B[0m\n\u001B[0;32m      1\u001B[0m nlp_data \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      2\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized_input\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m9\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m2\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;241m0\u001B[39m},\n\u001B[0;32m      3\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized_input\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m14\u001B[39m, \u001B[38;5;241m48\u001B[39m, \u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m23\u001B[39m, \u001B[38;5;241m154\u001B[39m, \u001B[38;5;241m2\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;241m0\u001B[39m},\n\u001B[0;32m      4\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized_input\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m30\u001B[39m, \u001B[38;5;241m67\u001B[39m, \u001B[38;5;241m117\u001B[39m, \u001B[38;5;241m21\u001B[39m, \u001B[38;5;241m15\u001B[39m, \u001B[38;5;241m2\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;241m1\u001B[39m},\n\u001B[0;32m      5\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized_input\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m17\u001B[39m, \u001B[38;5;241m2\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;241m0\u001B[39m},\n\u001B[0;32m      6\u001B[0m ]\n\u001B[0;32m      7\u001B[0m loader \u001B[38;5;241m=\u001B[39m DataLoader(nlp_data, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m----> 8\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\hehe\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\hehe\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\hehe\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\hehe\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    263\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\hehe\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:128\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMapping):\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 128\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m elem_type({key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem})\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    130\u001B[0m         \u001B[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001B[39;00m\n\u001B[0;32m    131\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem}\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\hehe\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:128\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMapping):\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 128\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m elem_type({key: \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43md\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem})\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    130\u001B[0m         \u001B[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001B[39;00m\n\u001B[0;32m    131\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem}\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\hehe\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:139\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    137\u001B[0m elem_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mnext\u001B[39m(it))\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28mlen\u001B[39m(elem) \u001B[38;5;241m==\u001B[39m elem_size \u001B[38;5;28;01mfor\u001B[39;00m elem \u001B[38;5;129;01min\u001B[39;00m it):\n\u001B[1;32m--> 139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meach element in list of batch should be of equal size\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    140\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n",
      "\u001B[1;31mRuntimeError\u001B[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "nlp_data = [\n",
    "    {'tokenized_input': [1, 4, 5, 9, 3, 2], 'label':0},\n",
    "    {'tokenized_input': [1, 7, 3, 14, 48, 7, 23, 154, 2], 'label':0},\n",
    "    {'tokenized_input': [1, 30, 67, 117, 21, 15, 2], 'label':1},\n",
    "    {'tokenized_input': [1, 17, 2], 'label':0},\n",
    "]\n",
    "loader = DataLoader(nlp_data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenized_input': tensor([[  1,   4,   5,   9,   3,   2,   0,   0,   0],\n",
      "        [  1,   7,   3,  14,  48,   7,  23, 154,   2]]), 'label': tensor([0, 0])}\n",
      "{'tokenized_input': tensor([[  1,  30,  67, 117,  21,  15,   2],\n",
      "        [  1,  17,   2,   0,   0,   0,   0]]), 'label': tensor([1, 0])}\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def custom_collate(data):\n",
    "    inputs = [torch.tensor(d['tokenized_input']) for d in data]\n",
    "    labels = [d['label'] for d in data]\n",
    "\n",
    "    inputs = pad_sequence(inputs, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {'tokenized_input': inputs,'label': labels}\n",
    "\n",
    "\n",
    "loader = DataLoader(nlp_data, batch_size=2, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "iter_loader = iter(loader)\n",
    "batch1 = next(iter_loader)\n",
    "print(batch1)\n",
    "batch2 = next(iter_loader)\n",
    "print(batch2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "img = torch.rand([100,100,3])\n",
    "\n",
    "caption_data = [\n",
    "    {'tokenized_input': torch.Tensor([1, 4, 5, 9, 3, 2]), 'image': img},\n",
    "    {'tokenized_input': torch.Tensor([1, 7, 3, 14, 48, 7, 23, 154, 2]), 'image': img},\n",
    "    {'tokenized_input': torch.Tensor([1, 30, 67, 117, 21, 15, 2]), 'image': img},\n",
    "    {'tokenized_input': torch.Tensor([1, 17, 2]), 'image': img},\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.9793, 0.7228, 0.1015],\n         [0.8943, 0.6690, 0.6087],\n         [0.9201, 0.5518, 0.0158],\n         ...,\n         [0.7315, 0.6927, 0.6763],\n         [0.3446, 0.1579, 0.1253],\n         [0.9792, 0.3437, 0.3341]],\n\n        [[0.1959, 0.9009, 0.3502],\n         [0.7227, 0.5673, 0.2331],\n         [0.4654, 0.3096, 0.3922],\n         ...,\n         [0.7267, 0.2307, 0.9683],\n         [0.0573, 0.4756, 0.9980],\n         [0.1970, 0.6830, 0.4833]],\n\n        [[0.9497, 0.2714, 0.9619],\n         [0.2788, 0.5174, 0.0272],\n         [0.5832, 0.5106, 0.9080],\n         ...,\n         [0.0483, 0.9736, 0.2991],\n         [0.7753, 0.1790, 0.8592],\n         [0.8276, 0.4556, 0.1739]],\n\n        ...,\n\n        [[0.0342, 0.5419, 0.6770],\n         [0.1287, 0.0173, 0.7120],\n         [0.3442, 0.6893, 0.6555],\n         ...,\n         [0.4775, 0.5405, 0.8124],\n         [0.7603, 0.0475, 0.9764],\n         [0.3287, 0.0549, 0.4437]],\n\n        [[0.2611, 0.6442, 0.5079],\n         [0.4211, 0.2508, 0.9381],\n         [0.5547, 0.3355, 0.0835],\n         ...,\n         [0.3105, 0.3468, 0.9545],\n         [0.2376, 0.4185, 0.0389],\n         [0.9514, 0.8854, 0.3451]],\n\n        [[0.7454, 0.8906, 0.8294],\n         [0.3368, 0.7786, 0.2922],\n         [0.7242, 0.5153, 0.6702],\n         ...,\n         [0.7749, 0.4421, 0.5911],\n         [0.1421, 0.5757, 0.1767],\n         [0.8182, 0.5530, 0.7174]]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_v2(batch):\n",
    "    imgs = [item['image'].unsqueeze(0) for item in batch]\n",
    "    img = torch.cat(imgs, dim=0)\n",
    "    targets = [item['tokenized_input'] for item in batch]\n",
    "    targets = pad_sequence(targets, batch_first=False)\n",
    "    return img, targets\n",
    "\n",
    "\n",
    "loader = DataLoader(caption_data, batch_size=2, shuffle=False, collate_fn=collate_v2)\n",
    "batch1 = next(iter(loader))\n",
    "batch1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[[0.9793, 0.7228, 0.1015],\n           [0.8943, 0.6690, 0.6087],\n           [0.9201, 0.5518, 0.0158],\n           ...,\n           [0.7315, 0.6927, 0.6763],\n           [0.3446, 0.1579, 0.1253],\n           [0.9792, 0.3437, 0.3341]],\n \n          [[0.1959, 0.9009, 0.3502],\n           [0.7227, 0.5673, 0.2331],\n           [0.4654, 0.3096, 0.3922],\n           ...,\n           [0.7267, 0.2307, 0.9683],\n           [0.0573, 0.4756, 0.9980],\n           [0.1970, 0.6830, 0.4833]],\n \n          [[0.9497, 0.2714, 0.9619],\n           [0.2788, 0.5174, 0.0272],\n           [0.5832, 0.5106, 0.9080],\n           ...,\n           [0.0483, 0.9736, 0.2991],\n           [0.7753, 0.1790, 0.8592],\n           [0.8276, 0.4556, 0.1739]],\n \n          ...,\n \n          [[0.0342, 0.5419, 0.6770],\n           [0.1287, 0.0173, 0.7120],\n           [0.3442, 0.6893, 0.6555],\n           ...,\n           [0.4775, 0.5405, 0.8124],\n           [0.7603, 0.0475, 0.9764],\n           [0.3287, 0.0549, 0.4437]],\n \n          [[0.2611, 0.6442, 0.5079],\n           [0.4211, 0.2508, 0.9381],\n           [0.5547, 0.3355, 0.0835],\n           ...,\n           [0.3105, 0.3468, 0.9545],\n           [0.2376, 0.4185, 0.0389],\n           [0.9514, 0.8854, 0.3451]],\n \n          [[0.7454, 0.8906, 0.8294],\n           [0.3368, 0.7786, 0.2922],\n           [0.7242, 0.5153, 0.6702],\n           ...,\n           [0.7749, 0.4421, 0.5911],\n           [0.1421, 0.5757, 0.1767],\n           [0.8182, 0.5530, 0.7174]]],\n \n \n         [[[0.9793, 0.7228, 0.1015],\n           [0.8943, 0.6690, 0.6087],\n           [0.9201, 0.5518, 0.0158],\n           ...,\n           [0.7315, 0.6927, 0.6763],\n           [0.3446, 0.1579, 0.1253],\n           [0.9792, 0.3437, 0.3341]],\n \n          [[0.1959, 0.9009, 0.3502],\n           [0.7227, 0.5673, 0.2331],\n           [0.4654, 0.3096, 0.3922],\n           ...,\n           [0.7267, 0.2307, 0.9683],\n           [0.0573, 0.4756, 0.9980],\n           [0.1970, 0.6830, 0.4833]],\n \n          [[0.9497, 0.2714, 0.9619],\n           [0.2788, 0.5174, 0.0272],\n           [0.5832, 0.5106, 0.9080],\n           ...,\n           [0.0483, 0.9736, 0.2991],\n           [0.7753, 0.1790, 0.8592],\n           [0.8276, 0.4556, 0.1739]],\n \n          ...,\n \n          [[0.0342, 0.5419, 0.6770],\n           [0.1287, 0.0173, 0.7120],\n           [0.3442, 0.6893, 0.6555],\n           ...,\n           [0.4775, 0.5405, 0.8124],\n           [0.7603, 0.0475, 0.9764],\n           [0.3287, 0.0549, 0.4437]],\n \n          [[0.2611, 0.6442, 0.5079],\n           [0.4211, 0.2508, 0.9381],\n           [0.5547, 0.3355, 0.0835],\n           ...,\n           [0.3105, 0.3468, 0.9545],\n           [0.2376, 0.4185, 0.0389],\n           [0.9514, 0.8854, 0.3451]],\n \n          [[0.7454, 0.8906, 0.8294],\n           [0.3368, 0.7786, 0.2922],\n           [0.7242, 0.5153, 0.6702],\n           ...,\n           [0.7749, 0.4421, 0.5911],\n           [0.1421, 0.5757, 0.1767],\n           [0.8182, 0.5530, 0.7174]]],\n \n \n         [[[0.9793, 0.7228, 0.1015],\n           [0.8943, 0.6690, 0.6087],\n           [0.9201, 0.5518, 0.0158],\n           ...,\n           [0.7315, 0.6927, 0.6763],\n           [0.3446, 0.1579, 0.1253],\n           [0.9792, 0.3437, 0.3341]],\n \n          [[0.1959, 0.9009, 0.3502],\n           [0.7227, 0.5673, 0.2331],\n           [0.4654, 0.3096, 0.3922],\n           ...,\n           [0.7267, 0.2307, 0.9683],\n           [0.0573, 0.4756, 0.9980],\n           [0.1970, 0.6830, 0.4833]],\n \n          [[0.9497, 0.2714, 0.9619],\n           [0.2788, 0.5174, 0.0272],\n           [0.5832, 0.5106, 0.9080],\n           ...,\n           [0.0483, 0.9736, 0.2991],\n           [0.7753, 0.1790, 0.8592],\n           [0.8276, 0.4556, 0.1739]],\n \n          ...,\n \n          [[0.0342, 0.5419, 0.6770],\n           [0.1287, 0.0173, 0.7120],\n           [0.3442, 0.6893, 0.6555],\n           ...,\n           [0.4775, 0.5405, 0.8124],\n           [0.7603, 0.0475, 0.9764],\n           [0.3287, 0.0549, 0.4437]],\n \n          [[0.2611, 0.6442, 0.5079],\n           [0.4211, 0.2508, 0.9381],\n           [0.5547, 0.3355, 0.0835],\n           ...,\n           [0.3105, 0.3468, 0.9545],\n           [0.2376, 0.4185, 0.0389],\n           [0.9514, 0.8854, 0.3451]],\n \n          [[0.7454, 0.8906, 0.8294],\n           [0.3368, 0.7786, 0.2922],\n           [0.7242, 0.5153, 0.6702],\n           ...,\n           [0.7749, 0.4421, 0.5911],\n           [0.1421, 0.5757, 0.1767],\n           [0.8182, 0.5530, 0.7174]]],\n \n \n         ...,\n \n \n         [[[0.9793, 0.7228, 0.1015],\n           [0.8943, 0.6690, 0.6087],\n           [0.9201, 0.5518, 0.0158],\n           ...,\n           [0.7315, 0.6927, 0.6763],\n           [0.3446, 0.1579, 0.1253],\n           [0.9792, 0.3437, 0.3341]],\n \n          [[0.1959, 0.9009, 0.3502],\n           [0.7227, 0.5673, 0.2331],\n           [0.4654, 0.3096, 0.3922],\n           ...,\n           [0.7267, 0.2307, 0.9683],\n           [0.0573, 0.4756, 0.9980],\n           [0.1970, 0.6830, 0.4833]],\n \n          [[0.9497, 0.2714, 0.9619],\n           [0.2788, 0.5174, 0.0272],\n           [0.5832, 0.5106, 0.9080],\n           ...,\n           [0.0483, 0.9736, 0.2991],\n           [0.7753, 0.1790, 0.8592],\n           [0.8276, 0.4556, 0.1739]],\n \n          ...,\n \n          [[0.0342, 0.5419, 0.6770],\n           [0.1287, 0.0173, 0.7120],\n           [0.3442, 0.6893, 0.6555],\n           ...,\n           [0.4775, 0.5405, 0.8124],\n           [0.7603, 0.0475, 0.9764],\n           [0.3287, 0.0549, 0.4437]],\n \n          [[0.2611, 0.6442, 0.5079],\n           [0.4211, 0.2508, 0.9381],\n           [0.5547, 0.3355, 0.0835],\n           ...,\n           [0.3105, 0.3468, 0.9545],\n           [0.2376, 0.4185, 0.0389],\n           [0.9514, 0.8854, 0.3451]],\n \n          [[0.7454, 0.8906, 0.8294],\n           [0.3368, 0.7786, 0.2922],\n           [0.7242, 0.5153, 0.6702],\n           ...,\n           [0.7749, 0.4421, 0.5911],\n           [0.1421, 0.5757, 0.1767],\n           [0.8182, 0.5530, 0.7174]]],\n \n \n         [[[0.9793, 0.7228, 0.1015],\n           [0.8943, 0.6690, 0.6087],\n           [0.9201, 0.5518, 0.0158],\n           ...,\n           [0.7315, 0.6927, 0.6763],\n           [0.3446, 0.1579, 0.1253],\n           [0.9792, 0.3437, 0.3341]],\n \n          [[0.1959, 0.9009, 0.3502],\n           [0.7227, 0.5673, 0.2331],\n           [0.4654, 0.3096, 0.3922],\n           ...,\n           [0.7267, 0.2307, 0.9683],\n           [0.0573, 0.4756, 0.9980],\n           [0.1970, 0.6830, 0.4833]],\n \n          [[0.9497, 0.2714, 0.9619],\n           [0.2788, 0.5174, 0.0272],\n           [0.5832, 0.5106, 0.9080],\n           ...,\n           [0.0483, 0.9736, 0.2991],\n           [0.7753, 0.1790, 0.8592],\n           [0.8276, 0.4556, 0.1739]],\n \n          ...,\n \n          [[0.0342, 0.5419, 0.6770],\n           [0.1287, 0.0173, 0.7120],\n           [0.3442, 0.6893, 0.6555],\n           ...,\n           [0.4775, 0.5405, 0.8124],\n           [0.7603, 0.0475, 0.9764],\n           [0.3287, 0.0549, 0.4437]],\n \n          [[0.2611, 0.6442, 0.5079],\n           [0.4211, 0.2508, 0.9381],\n           [0.5547, 0.3355, 0.0835],\n           ...,\n           [0.3105, 0.3468, 0.9545],\n           [0.2376, 0.4185, 0.0389],\n           [0.9514, 0.8854, 0.3451]],\n \n          [[0.7454, 0.8906, 0.8294],\n           [0.3368, 0.7786, 0.2922],\n           [0.7242, 0.5153, 0.6702],\n           ...,\n           [0.7749, 0.4421, 0.5911],\n           [0.1421, 0.5757, 0.1767],\n           [0.8182, 0.5530, 0.7174]]],\n \n \n         [[[0.9793, 0.7228, 0.1015],\n           [0.8943, 0.6690, 0.6087],\n           [0.9201, 0.5518, 0.0158],\n           ...,\n           [0.7315, 0.6927, 0.6763],\n           [0.3446, 0.1579, 0.1253],\n           [0.9792, 0.3437, 0.3341]],\n \n          [[0.1959, 0.9009, 0.3502],\n           [0.7227, 0.5673, 0.2331],\n           [0.4654, 0.3096, 0.3922],\n           ...,\n           [0.7267, 0.2307, 0.9683],\n           [0.0573, 0.4756, 0.9980],\n           [0.1970, 0.6830, 0.4833]],\n \n          [[0.9497, 0.2714, 0.9619],\n           [0.2788, 0.5174, 0.0272],\n           [0.5832, 0.5106, 0.9080],\n           ...,\n           [0.0483, 0.9736, 0.2991],\n           [0.7753, 0.1790, 0.8592],\n           [0.8276, 0.4556, 0.1739]],\n \n          ...,\n \n          [[0.0342, 0.5419, 0.6770],\n           [0.1287, 0.0173, 0.7120],\n           [0.3442, 0.6893, 0.6555],\n           ...,\n           [0.4775, 0.5405, 0.8124],\n           [0.7603, 0.0475, 0.9764],\n           [0.3287, 0.0549, 0.4437]],\n \n          [[0.2611, 0.6442, 0.5079],\n           [0.4211, 0.2508, 0.9381],\n           [0.5547, 0.3355, 0.0835],\n           ...,\n           [0.3105, 0.3468, 0.9545],\n           [0.2376, 0.4185, 0.0389],\n           [0.9514, 0.8854, 0.3451]],\n \n          [[0.7454, 0.8906, 0.8294],\n           [0.3368, 0.7786, 0.2922],\n           [0.7242, 0.5153, 0.6702],\n           ...,\n           [0.7749, 0.4421, 0.5911],\n           [0.1421, 0.5757, 0.1767],\n           [0.8182, 0.5530, 0.7174]]]]),\n tensor([[  1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.],\n         [  4.,   7.,  30.,  17.,   4.,   7.,  30.,  17.],\n         [  5.,   3.,  67.,   2.,   5.,   3.,  67.,   2.],\n         [  9.,  14., 117.,   0.,   9.,  14., 117.,   0.],\n         [  3.,  48.,  21.,   0.,   3.,  48.,  21.,   0.],\n         [  2.,   7.,  15.,   0.,   2.,   7.,  15.,   0.],\n         [  0.,  23.,   2.,   0.,   0.,  23.,   2.,   0.],\n         [  0., 154.,   0.,   0.,   0., 154.,   0.,   0.],\n         [  0.,   2.,   0.,   0.,   0.,   2.,   0.,   0.]]))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
