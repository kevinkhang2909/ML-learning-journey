{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1000,  7.4000,  0.0000],\n        [-0.2000,  5.3000,  0.0000]], dtype=torch.float64)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = np.array([\n",
    "    [0.1, 7.4, 0],\n",
    "    [-0.2, 5.3, 0],\n",
    "    [0.2, 8.2, 1],\n",
    "    [0.2, 7.7, 1]\n",
    "])\n",
    "loader = DataLoader(data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{'x1': tensor([ 0.1000, -0.2000], dtype=torch.float64),\n 'x2': tensor([7.4000, 5.3000], dtype=torch.float64),\n 'y': tensor([0, 0])}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_data = [\n",
    "    {'x1': 0.1, 'x2': 7.4, 'y': 0},\n",
    "    {'x1': -0.2, 'x2': 5.3, 'y': 0},\n",
    "    {'x1': 0.2, 'x2': 8.2, 'y': 1},\n",
    "    {'x1': 0.2, 'x2': 7.7, 'y': 10},\n",
    "]\n",
    "loader = DataLoader(dict_data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m nlp_data \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      2\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized_input\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m9\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m2\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;241m0\u001B[39m},\n\u001B[1;32m      3\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized_input\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m14\u001B[39m, \u001B[38;5;241m48\u001B[39m, \u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m23\u001B[39m, \u001B[38;5;241m154\u001B[39m, \u001B[38;5;241m2\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;241m0\u001B[39m},\n\u001B[1;32m      4\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized_input\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m30\u001B[39m, \u001B[38;5;241m67\u001B[39m, \u001B[38;5;241m117\u001B[39m, \u001B[38;5;241m21\u001B[39m, \u001B[38;5;241m15\u001B[39m, \u001B[38;5;241m2\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;241m1\u001B[39m},\n\u001B[1;32m      5\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized_input\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m17\u001B[39m, \u001B[38;5;241m2\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;241m0\u001B[39m},\n\u001B[1;32m      6\u001B[0m ]\n\u001B[1;32m      7\u001B[0m loader \u001B[38;5;241m=\u001B[39m DataLoader(nlp_data, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m----> 8\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/hihi/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.conda/envs/hihi/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/.conda/envs/hihi/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:61\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/hihi/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001B[0m, in \u001B[0;36mdefault_collate\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/hihi/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:128\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMapping):\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 128\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m elem_type({key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem})\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001B[39;00m\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem}\n",
      "File \u001B[0;32m~/.conda/envs/hihi/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:128\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMapping):\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 128\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m elem_type({key: \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43md\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem})\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001B[39;00m\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem}\n",
      "File \u001B[0;32m~/.conda/envs/hihi/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:139\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    137\u001B[0m elem_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mnext\u001B[39m(it))\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28mlen\u001B[39m(elem) \u001B[38;5;241m==\u001B[39m elem_size \u001B[38;5;28;01mfor\u001B[39;00m elem \u001B[38;5;129;01min\u001B[39;00m it):\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meach element in list of batch should be of equal size\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    140\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n",
      "\u001B[0;31mRuntimeError\u001B[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "nlp_data = [\n",
    "    {'tokenized_input': [1, 4, 5, 9, 3, 2], 'label':0},\n",
    "    {'tokenized_input': [1, 7, 3, 14, 48, 7, 23, 154, 2], 'label':0},\n",
    "    {'tokenized_input': [1, 30, 67, 117, 21, 15, 2], 'label':1},\n",
    "    {'tokenized_input': [1, 17, 2], 'label':0},\n",
    "]\n",
    "loader = DataLoader(nlp_data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenized_input': tensor([[  1,   4,   5,   9,   3,   2,   0,   0,   0],\n",
      "        [  1,   7,   3,  14,  48,   7,  23, 154,   2]]), 'label': tensor([0, 0])}\n",
      "{'tokenized_input': tensor([[  1,  30,  67, 117,  21,  15,   2],\n",
      "        [  1,  17,   2,   0,   0,   0,   0]]), 'label': tensor([1, 0])}\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def custom_collate(data):\n",
    "    inputs = [torch.tensor(d['tokenized_input']) for d in data]\n",
    "    labels = [d['label'] for d in data]\n",
    "\n",
    "    inputs = pad_sequence(inputs, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {'tokenized_input': inputs,'label': labels}\n",
    "\n",
    "\n",
    "loader = DataLoader(nlp_data, batch_size=2, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "iter_loader = iter(loader)\n",
    "batch1 = next(iter_loader)\n",
    "print(batch1)\n",
    "batch2 = next(iter_loader)\n",
    "print(batch2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "img = torch.rand([100,100,3])\n",
    "\n",
    "caption_data = [\n",
    "    {'tokenized_input': torch.Tensor([1, 4, 5, 9, 3, 2]), 'image': img},\n",
    "    {'tokenized_input': torch.Tensor([1, 7, 3, 14, 48, 7, 23, 154, 2]), 'image': img},\n",
    "    {'tokenized_input': torch.Tensor([1, 30, 67, 117, 21, 15, 2]), 'image': img},\n",
    "    {'tokenized_input': torch.Tensor([1, 17, 2]), 'image': img},\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[[0.1041, 0.2867, 0.3715],\n           [0.4931, 0.8088, 0.2303],\n           [0.3669, 0.2837, 0.0822],\n           ...,\n           [0.3211, 0.9675, 0.4290],\n           [0.8574, 0.9144, 0.8274],\n           [0.5688, 0.2943, 0.0994]],\n \n          [[0.5970, 0.7365, 0.6103],\n           [0.3946, 0.5017, 0.0118],\n           [0.5527, 0.0566, 0.7331],\n           ...,\n           [0.7194, 0.5310, 0.6839],\n           [0.4039, 0.2468, 0.8682],\n           [0.4612, 0.5002, 0.9753]],\n \n          [[0.6160, 0.0323, 0.6637],\n           [0.1150, 0.9734, 0.4589],\n           [0.6361, 0.9561, 0.1510],\n           ...,\n           [0.9925, 0.8388, 0.8565],\n           [0.0661, 0.9182, 0.3989],\n           [0.7344, 0.2131, 0.7458]],\n \n          ...,\n \n          [[0.1396, 0.2027, 0.4563],\n           [0.7817, 0.1654, 0.7436],\n           [0.6120, 0.3277, 0.9097],\n           ...,\n           [0.7917, 0.9720, 0.0761],\n           [0.4049, 0.7703, 0.1730],\n           [0.5654, 0.4673, 0.3991]],\n \n          [[0.3920, 0.4337, 0.7494],\n           [0.4064, 0.1229, 0.5764],\n           [0.7465, 0.3049, 0.1025],\n           ...,\n           [0.5768, 0.0149, 0.5440],\n           [0.3431, 0.1769, 0.6612],\n           [0.8334, 0.2545, 0.7242]],\n \n          [[0.0619, 0.8926, 0.7068],\n           [0.3078, 0.2424, 0.6383],\n           [0.6434, 0.3222, 0.9324],\n           ...,\n           [0.6482, 0.2762, 0.8029],\n           [0.4056, 0.4926, 0.6911],\n           [0.0896, 0.5915, 0.3369]]],\n \n \n         [[[0.1041, 0.2867, 0.3715],\n           [0.4931, 0.8088, 0.2303],\n           [0.3669, 0.2837, 0.0822],\n           ...,\n           [0.3211, 0.9675, 0.4290],\n           [0.8574, 0.9144, 0.8274],\n           [0.5688, 0.2943, 0.0994]],\n \n          [[0.5970, 0.7365, 0.6103],\n           [0.3946, 0.5017, 0.0118],\n           [0.5527, 0.0566, 0.7331],\n           ...,\n           [0.7194, 0.5310, 0.6839],\n           [0.4039, 0.2468, 0.8682],\n           [0.4612, 0.5002, 0.9753]],\n \n          [[0.6160, 0.0323, 0.6637],\n           [0.1150, 0.9734, 0.4589],\n           [0.6361, 0.9561, 0.1510],\n           ...,\n           [0.9925, 0.8388, 0.8565],\n           [0.0661, 0.9182, 0.3989],\n           [0.7344, 0.2131, 0.7458]],\n \n          ...,\n \n          [[0.1396, 0.2027, 0.4563],\n           [0.7817, 0.1654, 0.7436],\n           [0.6120, 0.3277, 0.9097],\n           ...,\n           [0.7917, 0.9720, 0.0761],\n           [0.4049, 0.7703, 0.1730],\n           [0.5654, 0.4673, 0.3991]],\n \n          [[0.3920, 0.4337, 0.7494],\n           [0.4064, 0.1229, 0.5764],\n           [0.7465, 0.3049, 0.1025],\n           ...,\n           [0.5768, 0.0149, 0.5440],\n           [0.3431, 0.1769, 0.6612],\n           [0.8334, 0.2545, 0.7242]],\n \n          [[0.0619, 0.8926, 0.7068],\n           [0.3078, 0.2424, 0.6383],\n           [0.6434, 0.3222, 0.9324],\n           ...,\n           [0.6482, 0.2762, 0.8029],\n           [0.4056, 0.4926, 0.6911],\n           [0.0896, 0.5915, 0.3369]]]]),\n tensor([[  1.,   1.],\n         [  4.,   7.],\n         [  5.,   3.],\n         [  9.,  14.],\n         [  3.,  48.],\n         [  2.,   7.],\n         [  0.,  23.],\n         [  0., 154.],\n         [  0.,   2.]]))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_v2(batch):\n",
    "    imgs = [item['image'].unsqueeze(0) for item in batch]\n",
    "    img = torch.cat(imgs, dim=0)\n",
    "    targets = [item['tokenized_input'] for item in batch]\n",
    "    targets = pad_sequence(targets, batch_first=False)\n",
    "    return img, targets\n",
    "\n",
    "\n",
    "loader = DataLoader(caption_data, batch_size=2, shuffle=False, collate_fn=collate_v2)\n",
    "batch1 = next(iter(loader))\n",
    "batch1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
